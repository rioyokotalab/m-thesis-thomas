\section{IR with Mixed Precision}
\label{sec:mixed_precision}

As a direct consequence of the discussion about the relation between the working precision and the condition number of the matrix $A$ in Chapter~\hyperref[chap:iterative_refinement]{\ref{chap:iterative_refinement}} (see Table~\hyperref[tab:ir_bounds]{\ref{tab:ir_bounds}}) the convergence of LU-based iterative refinement is bound by the reciprocal of the working precision $u$. In the case of double precision ($u \approx 10^{-16}$), this translates to $\kappa_\infty(A) \leq 10^{16}$ as the maximum condition number for which convergence of the algorithm can be expected. This fact can be observed on the simple test case of a $100 \times 100$ matrix illustrated in Figure~\hyperref[fig:lue_ir]{\ref{fig:lue_ir}}.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{chapters/5_experiments/figures/LUe16.pdf}
  \caption{Convergence for $\kappa_\infty(A) = 10^{16}$}
  \label{fig:lue16}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{chapters/5_experiments/figures/LUe18.pdf}
  \caption{Divergence for $\kappa_\infty(A) = 10^{18}$}
  \label{fig:lue18}
\end{subfigure}
\caption[LU-based IR]{Convergence behaviour of LU-based iterative refinement for different condition numbers. Comparison between the MATLAB implementation from \cite{carson_new_2017} and the new C++ code (ferr = forward error, nbe = norm-wise backward error, cbe = component-wise backward error).}
\label{fig:lue_ir}
\end{figure}

The matrix was created using the build-in MATLAB function \textit{gallery('randsvd',kappa, mode)}, where \textit{kappa} specifies the 2-norm condition number and the parameter \textit{mode} refers to the distribution of the singular values (in this case $mode=2$ was used). The plots show the behaviour of the forward error (ferr, in red), norm-wise relative backward error (nbe in blue) and component-wise relative backward error (cbe in green). The dotted black line indicates the accuracy of the working precision $u$. The solver is LU factorization computed in the working precision (double), while the residuals are accumulated in quadruple precision.

As expected from such an ill-conditioned matrix, the forward error of the initial solution is quite large and close to $1$, while both backward errors remain small. If the condition number remains within the upper bound previously mentioned, iterative refinement is able to reduce the error in each iteration, converging to a solution (almost) accurate to precision $u$. This behaviour is depicted in Figure~\hyperref[fig:lue16]{\ref{fig:lue16}}. When the condition number becomes to large however, the solution starts to diverge and LU-based IR is no longer beneficial, as is shown in the case of Figure~\hyperref[fig:lue18]{\ref{fig:lue18}} (note that a condition number of $10^{18}$ has been chosen for clarity, $10^{17}$ would be sufficient to evoke this behaviour).

In order to obtain accurate condition numbers even for very ill-conditioned systems, \cite{carson_new_2017} introduced GMRES-based iterative refinement. In this approach, the LU factors are only used as a preconditioner to a modified GMRES variant, enabling the correction equation to be solved at a higher accuracy. Doing this removes the direct dependency on the precision of the LU factors, which consequently relaxes the constraint on the condition number. The convergence behaviour for this algorithm (on the same test matrices) is given in Figure~\hyperref[fig:gmrese_ir]{\ref{fig:gmrese_ir}}.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{chapters/5_experiments/figures/GMRESe16.pdf}
  \caption{Convergence for $\kappa_\infty(A) = 10^{16}$}
  \label{fig:gmrese16}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{chapters/5_experiments/figures/GMRESe18.pdf}
  \caption{Convergence for $\kappa_\infty(A) = 10^{18}$}
  \label{fig:gmrese18}
\end{subfigure}
\caption[GMRES-based IR]{Convergence behaviour of GMRES-based iterative refinement for different condition numbers. Comparison between the MATLAB implementation from \cite{carson_new_2017} and the new C++ code (ferr = forward error, nbe = norm-wise backward error, cbe = component-wise backward error).}
\label{fig:gmrese_ir}
\end{figure}

There are two important observations to be made. First, as can be seen from Figure~\hyperref[fig:gmrese18]{\ref{fig:gmrese18}}, the algorithm converges to a solution accurate to working precision (i.e. double) even if $\kappa_\infty(A) > 10^{16}$. Therefore this approach is promising for very ill-conditioned systems, even though the boundaries for convergence remain (currenly) unclear (see \cite{carson_new_2017}). However, it has been demonstrated by \cite{carson_accelerating_2018}, that this method is able to tolerate LU factors that are computed at half the working precision. 

Secondly, as revealed by a comparison of Figure~\hyperref[fig:lue_ir]{\ref{fig:lue_ir}} \& \hyperref[fig:gmrese18_ir]{\ref{fig:gmrese_ir}}, the necessary number of iterative refinement steps is reduced when compared to the LU-based algorithm. Despite a lower number of steps, the GMRES-based variant generally takes longer to execute, because the true cost is hidden within the GMRES iterations. When the convergence tolerance of GMRES is set to $10^{-4}$ (as proposed in \cite{carson_new_2017}), the average number of GMRES iterations for the above test matrices is $16$ for each IR step. Consequently, performance is highly dependent on the convergence of GMRES and thus on the distribution of the singular values. Hence, for most practical use-cases, the increased numerical stability is associated with a higher runtime and unless the system is very ill-conditioned standard LU-based iterative refinement is usually the preferable choice.

Nonetheless, due to its tolerance for low-precision LU factors, the method might still be beneficial for general systems, if the iterative refinement process is generalized to three precisions. In this case, the reduced cost of the factorization might be enough to offset the additional costs, resulting in a better performance overall. Since traditional iterative refinement is directly dependent on the accuracy of the factorization, its usefulness for low-precision calculations is limited as well. The examples provided in Figure~\hyperref[fig:lu_ir3]{\ref{fig:lu_ir3}} demonstrate the breakdown of IR for a condition number of $\kappa_\infty (A)=10^9$, if the factorization is calculated in single precision. It is noteworthy, that this restriction is independent of how the other precisions are chosen and cannot be offset by, for example, calculating the residuals in twice the working precision.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{chapters/5_experiments/figures/LUsdd.pdf}
  \caption{LU-IR (single - double - double)}
  \label{fig:lusdd}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{chapters/5_experiments/figures/LUsdq.pdf}
  \caption{LU-IR (single - double - quadruple)}
  \label{fig:lusdq}
\end{subfigure}
\caption[LU-based IR (single precision factorization)]{Convergence behaviour of LU-based iterative refinement for a condition number of $\kappa_\infty (A)=10^9$, when the LU factors are calculated in single precision. Comparison between the MATLAB implementation from \cite{carson_new_2017} and the new C++ code (ferr = forward error, nbe = norm-wise backward error, cbe = component-wise backward error).}
\label{fig:lu_ir3}
\end{figure}

GMRES-based iterative refinement, on the other hand, is able to achieve a solution accurate to working precision, even if only single precision LU factors are available. As can be observed in Figure~\hyperref[fig:gmres_ir3]{\ref{fig:gmres_ir3}}, the accuracy of the obtained solution by GMRES-IR in 3 precisions is equivalent to traditional iterative refinement in double precision. The results in Figure~\hyperref[fig:gmressdq]{\ref{fig:gmressdq}} where obtained by using a tolerance of $10^{-6}$ for the GMRES part of the algorithm, which converged after $2-3$ in each IR step. Hence, the actual performance overhead should not be too large and this variant might be beneficial on systems where fast single precision arithmetics are available.

\begin{figure}
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{chapters/5_experiments/figures/LUddq.pdf}
  \caption{LU-IR (double - double - quadruple)}
  \label{fig:luddq}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=\linewidth]{chapters/5_experiments/figures/GMRESsdq.pdf}
  \caption{GMRES-IR (single - double - quadruple)}
  \label{fig:gmressdq}
\end{subfigure}
\caption[GMRES based IR in 3 Precisions]{Convergence behaviour of GMRES-based iterative refinement for a condition number of $\kappa_\infty (A)=10^9$, when the LU factors are calculated in single precision in contrast to LU-based double precision IR. Comparison between the MATLAB implementation from \cite{carson_new_2017} and the new C++ code (ferr = forward error, nbe = norm-wise backward error, cbe = component-wise backward error).}
\label{fig:gmres_ir3}
\end{figure}

For iterative refinement in two precisions (single and double), \cite{langou_exploiting_2006} demonstrated that on certain microprocessor architectures, a speed-up of up to $4.7$ can be observed. This routine is available as a LAPACK function under the name \textit{DSGESV}, being equivalent to LU-based iterative refinement with precisions single - double - double. However, for iterative refinement in three precisions, similar performance analyses are not yet available. This thesis aims to close this gap in research by comparing the performance of the following methods:
\begin{itemize}
    \item \textit{DGESV}: in double precision (LAPACK routine that solves a linears system directly via LU factorization)
    \item \textit{DSGESV}: single - double - double precision (LAPACK routine based on iterative refinement of a single precision LU factorization)
    \item \textit{LU-IR (sdd)}: single - double - double precision (same as DDGESV but implemented in this research)
    \item \textit{LU-IR (ddq)}: double - double - quadruple precision (LU-based IR with double precision factorization, implemented in this research)
    \item \textit{GMRES-IR (sdq)}: single - double - quadruple precision (GMRES based IR with single precision factorization, implemented in this research)
\end{itemize}

\noindent But before going into details about the runtime, it is important to confirm the error bound of the different algorithms. For this purpose, a $512 \times 512$ matrix is used created via the \textit{gallery} function using $mode = 1$ (i.e. one large singular value). Since both backward errors stay safely below the working precision (i.e. $\leq 10^{-16}$) if convergence is achieved, this analysis will be focused mainly on the forward error. The results obtained for this comparison are illustrated in Figure~\hyperref[fig:ir3_ac]{\ref{fig:ir3_ac}}. A few remarks have to be made with regards to the interpretation of those experiments. First, all measurements below $\leq 10^{-20}$ have been clipped to the same value to maintain readability of the figure. Second, the errors achieved by \textit{DSGESV} and \textit{LU-IR (sdd)} are almost identical and the latter has been therefore omitted. Third, if it does not converge within 30 iterations \textit{DSGESV} falls back to the \textit{DGESV} routine in double precision making the results identical. Therefore, no error is reported if \textit{DSGESV} fails to achieve convergence.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{chapters/5_experiments/figures/IR3_acc.pdf}
    \caption[IR - Forward Error 1]{Forward error analysis of different routines for solving a linear system. Note that DSGESV does not converge for $\kappa_\infty(A)=10^8$.}
    \label{fig:ir3_ac}
\end{figure}

Generally, it can be observed that the forward error of a direct solving approach (\textit{DGESV}) increases with the condition number of $A$. However, up to a condition number of $u^{-1}$, this can be countered by using a few iterative refinement steps to obtain a solution accurate to working precision (\textit{LU-IR (ddq)}). If the factorization is computed in single precision (\textit{DSGESV} and \textit{GMRES-IR (sdq)}), traditional LU-based iterative refinement fails as the condition number approaches the reciprocal of the factorization precision (i.e. $\approx 10^8$) and both the error as well as the number of iterations increases with the condition number. In contrast, GMRES-based IR is able to maintain a solution accurate to working precision, even if $A$ becomes increasingly ill-conditioned.

It is worth mentioning that the the achieved accuracy can be guaranteed independent of the matrix size $n$. As shown in Figure~\hyperref[fig:ir3_n]{\ref{fig:ir3_n}}, the error remains relatively constant for the different routines, depending solely on the condition number of $A$. Most interestingly, the number of iterative refinement steps necessary follows a similar trend, remaining constant with regards to the matrix size. A similar observation can be made for the number of GMRES iterations, even if the variation is slightly larger in this case.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.9\linewidth]{chapters/5_experiments/figures/IR3_n.pdf}
    \caption[IR - Forward Error 2]{Forward error analysis with respect to the matrix size $n$ for $\kappa_\infty(A)=10^8$. The accuracy remains relatively constant.}
    \label{fig:ir3_n}
\end{figure}