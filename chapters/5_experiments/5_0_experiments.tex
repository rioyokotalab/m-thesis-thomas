\chapter{Experiments}
\label{chap:experiments}

The iterative refinement method implemented in this research is based on the findings of \cite{carson_new_2017}, providing the fundamental groundwork for obtaining accurate solutions even for ill-conditioned matrices. This approach was extended in \cite{carson_accelerating_2018} to iterative refinement in three precisions, providing a MATLAB implementation for numerical experiments. However, while invaluable for the purpose of numerical analysis and research, such implementations are hardly used for applications in high-performance computing due to their performance limitations. In such a setting, it is common to rely on the fast linear algebra routines provided by highly optimized kernels from libraries like BLAS or LAPACK which are known for their efficiency and numerical stability. Those routines have been fine-tuned and optimized for many years on different central processing units (CPUs), but until recently did not consider mixed precision algorithms. Due to the increasing memory gap and parallelization efforts on current computing architectures, which gave rise to a number of specialised processing units (e.g. TPU), efforts have been made to incorporate mixed precision methods. For example, the single-double precision iterative refinement proposed by \cite{langou_exploiting_2006}, is now available in the LAPACK routine \textit{DSGESV}, aimed at architectures featuring high single precision arithmetic performance. Additionally, this  algorithm hass the advantage of reducing data movement by half, thus reducing communication (on distributed systems) and increasing cache performance. 

Nonetheless, to the author's best knowledge, there  are currently no high-performance implementations of iterative refinement in three precisions for CPUs. The first section of this chapter will thus be dedicated to such an implementation, hoping to establish a common baseline for future research. This includes verification of the numerical results obtained by \cite{carson_accelerating_2018}, as well as a performance analysis compared to traditional approaches. The goal is to establish a mixed precision framework based on optimized LAPACK/BLAS routines that is able to deliver both high accuracy and performance.

This implementation will then be used as a reference for extending the method to hierarchical matrices in the Section~\hyperref[sec:low_rank]{\ref{sec:low_rank}}. Common low-rank approximation techniques will be used to replace the low-precision factorization step in the IR algorithm in order to reduce the overall complexity down to $\mathcal{O}(n^2)$. By controlling the accuracy of the approximation, it should be possible to guarantee error-bounds similar to the general case and this hypothesis is investigated empirically. To gain further insights into the capability of low-rank representations to maintain information that is crucial for iterative refinement, experiments will be conducted for various cases of condition number and accuracy.

The last sections will then be dedicated to the combination of hierarchical matrices with low-precision and parallel implementations, investigating whether or not there is potential for additional optimization. First, it will be investigated if the proposed hierarchical iterative refinement method can be integrated into a three precision setting, where the factorization is computed in half the working precision. This would not only speed up the decomposition, but additionally reduce the overhead for creating the low-rank representations, as those calculations can be done in lower precision. The parallel implementation finally serves as a baseline to analyze parallelization opportunities in iterative refinement. Parallelism is an essential criteria for achieving high performance on modern supercomputers and the ultimate goal that this research strives to achieve.

Unless otherwise noted, all experiments in the following sections were conducted on an AMD EPYC 7232P processor featuring 8 cores and 16 threads. This architecture features a 512 KiB L1 cache, a 4 MiB L4 cache and a 32 MiB L3 cache. The source code was compiled with the GNU C++ compiler, enabling optimizations via the \textit{O3} flag. For matrix operations, the BLAS and LAPACK routines provided in the Intel math kernel library (MKL, 2020 version) were called for the single/double precision parts of the calculations. For the hierarchical computations the HiCMA library developed at Rio Yokota Lab (Tokyo Insitute of Technology) was employed. Finally, the QD library developed by \cite{hida_quad-double_2001} (version 2.3.22) was used for the quadruple precision calculations that become necessary to obtain results accurate to double precision.

For iterative refinement in three precisions (see Chapter~\hyperref[chap:iterative_refinement]{\ref{chap:iterative_refinement}}), the decision was made to focus on the $single$ - $double$ - $quadruple$ case for comparability reasons. Unfortunately, half precision is currently not supported by HiCMA and it would have been difficult to relate the obtained results otherwise. As a result, the time measurements within this research should be considered carefully, as they do not represent the full capabilities of all precisions being supported in hardware.

With regards to the approximate solution obtained by the algorithms, the following error measurements will be used (where $x$ denotes the exact solution):

\begin{itemize}
    \item Forward Error (\textit{ferr}):
    \begin{equation}
        ferr(\hat{x}) = \frac{\norm{\hat{x}-x}_\infty}{\norm{\hat{x}_\infty}}
    \end{equation}
    \item Norm-wise Backward Error (\textit{nbe}):
    \begin{equation}
        nbe(\hat{x}) = \frac{\norm{A\hat{x}-b}_\infty}{\norm{A}_\infty\norm{\hat{x}}_\infty+\norm{b}_\infty}
    \end{equation}
    \item Component-wise Backward Error (\textit{cbe}):
    \begin{equation}
        cbe(\hat{x}) = max_i \left(\frac{|A\hat{x}-b|_i}{(|A||\hat{x}|+|b|)_i}\right)
    \end{equation}
\end{itemize}





\import{}{5_1_mixed_precision.tex}
\import{}{5_2_low_rank.tex}
\import{}{5_3_combination.tex}
\import{}{5_4_parallel_implementation.tex}