\chapter{Introduction}
\label{chap:introduction}


Finding the solution to a system of equations is the central problem of linear algebra \cite{strang_introduction_2009} and a fundamental building block of many scientific and engineering applications \cite{carson_new_2017}. Especially discretization techniques used in physics to solve (partial) differential and integral equations (such as the boundary element method and Nystrom method) are known to produce large systems of linear equations, requiring considerable computational resources in order to be solved. In some cases, the size of the resulting matrix (representing the linear system) becomes prohibitively large and thus too computationally expensive, even for modern supercomputers. 

However, many practical problems feature characteristics than can be exploited to achieve faster computations, facilitating the processing of such large systems. For example, positive-definite symmetric matrices can be solved in roughly half the time using Cholesky-factorization and many algorithms can and have been modified to take advantage of sparsity \cite{davis_direct_2006}. Other techniques such as low-rank approximations aim to reduce the problem size by using a smaller, approximate representation of the original matrix, in principle sacrificing accuracy for speed of the calculations. Based on the observation that data originating from natural processes is inadvertently associated with a measurement error, it is often sufficient for the solution to match the accuracy of the original data. 

In such a case, one might even consider to reduce the precision of the underlying floating-point format according to the accuracy requirements. It is well known that in the IEEE floating-point standard \cite{institute_of_electrical_and_electronics_engineers_ieee_2008} single precision arithmetic is twice as fast as double precision \cite{buttari_mixed_2007} and even half-precision is now increasingly becoming available in hardware (e.g. NVIDIA P100 and V100 GPUs as well as AMD Radeon Instinct MI25 GPU), together with specialized matrix multiplication units such as NVIDIA Tensor Cores or Google's Tensor Processing Units. Making such hardware advantages available to problems demanding a higher level of accuracy is the goal of mixed-precision algorithms, for example as implemented in the LAPACK DSGESV \cite{buttari_mixed_2007} subroutine for solving linear systems. This thesis is dedicated to the purpose of extending the research in that area, investigating the application of hierarchical low-rank approximations for the solution of general dense linear system in terms of achievable speed-up and and accuracy. 

\import{}{1_1_background.tex}
\import{}{1_2_related_work.tex}
\import{}{1_3_objectives.tex}
\import{}{1_4_contributions.tex}
\import{}{1_5_structure.tex}
