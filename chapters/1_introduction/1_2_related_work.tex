\section{Related Work}
\label{sec:related_work}

A specialized form of an iterative method, the so-called \textit{iterative refinement} was developed and published by Wilkinson \cite{wilkinson_rounding_1963} in order to tackle the aforementioned accuracy issues when solving an ill-conditioned linear system. In principle, the algorithm computes the residual error of the current solution, calculates a a correction term and then updates the current solution to generate a new, increasingly accurate one in each iteration. In this initial form of iterative refinement, the residual error was calculated in twice the working precision and LU factorization was used for solving the system. The first error analyses for this technique were given by Wilkinson \cite{wilkinson_rounding_1963} (for fixed point arithmetic) and Moler \cite{moler_iterative_1967} (for general floating-point calculations), proving that if $A$ is not too ill-conditioned, a solution accurate to working precision can be achieved.

\cite{skeel_iterative_1980} was one of the first to investigate fixed precision iterative refinement, where the residuals are calculated in working precision, showing that under certain conditions just one iteration is enough to yield a small component-wise backward error for Gaussian elimination with partial pivoting (i.e. LU with row or column pivoting). This results were later extended for general solvers by \cite{higham_iterative_1991} and \cite{higham_iterative_1997}, providing the theoretical background to justify the use of fixed precision iterative refinement in several LAPACK \cite{anderson_lapack_1999} routines.

Based on the observation that on modern CPU architectures, single precision calculations are at least twice as fast as double precision ones, Lagou \cite{langou_exploiting_2006} revisited traditional iterative refinement with the goal of increasing performance during the factorization step. Their method solves the system entirely in single precision, while both calculating the residual and updating the current solution remain in double precision. As a result, the most expensive part of the algorithm (i.e. the factorization which has $\mathcal{O}(n^3)$) can be done at twice the speed, at the cost of a few additional refinement steps and achieve results that are accurate to double precision. Consequently, single/double mixed precision iterative refinement for linear solvers has been implemented in LAPACK \cite{anderson_lapack_1999} and has been extensively exploited in its spiritual successors PLASMA and MAGMA \cite{abdelfattah_linear_2016}. For direct solving techniques, the authors report a speed-up of 7 on the CPU and 26 on the GPU for linear system where the condition number of the coefficient matrix is smaller then the reciprocal of the unit round-off for the lowest precision used (see background). The corresponding error bounds for the algorithm used are provided by \cite{arioli_using_2008}.

Higham \cite{carson_new_2017} were one of the first to show that iterative refinement can be applied successfully, even if $\kappa(A) \geq u^{-1}$, provided that the update equation can be solved with a relative error sufficiently less than 1. Their approach is based on the observation that the calculated LU factors contain useful information even for $\kappa(A) \gg u^{-1}$, since $\kappa(L^{-1}AU^{-1}) \approx 1+\kappa(A)u$ (see \cite{rump_approximate_1990}, \cite{rump_inversion_2009}). By making use of the generalized minimal residual method (GMRES) \cite{saad_gmres_1986} preconditioned by the LU factors to solve for the correction term, their approach can succeed, even when traditional iterative refinement (using the LU factors to solve $Ad=r$) fails.

This approach is later extended in \cite{carson_accelerating_2018}, making use of the characteristics mentioned above to facilitate iterative refinement in three precisions. In this method, the factorization is computed in lower precision, while still obtaining a result accurate to working precision, even if the matrix is ill-conditioned. Hence, the part of the algorithm requiring $\mathcal{O}(n^3)$ can be approximately twice as fast in half of the working precision, while the residual has to be calculated in higher precision (usually twice the working precision). Their work demonstrates that if the precisions of the algorithm are set to \textit{half - single - double} (or \textit{single - double - quadruple}), the accuracy of the result remains accurate to \textit{single} (\textit{double}) precision for $\kappa(A) \leq u^{-1}$ (where $u$ is the working precision). Especially with the rise of specialized low-precision hardware (e.g. NVIDIA Tensor Cores), mixed precision algorithms that are able to exploit such devices become increasingly attractive. By implementing this algorithm (with slight modifications to accomodate for the underlying hardware) on a NVIDIA V100 GPU, \cite{haidar_harnessing_2018}, report a speedup of 4 when compared to a regular solver in double precision (achieving the same accuracy).

To accommodate for even more ill-conditioned linear systems, both a refined preconditioner an a more general framework were proposed in \cite{higham_new_2019}. The new preconditioning involves the error matrix to the (approximate) factorization $E=U^{-1}L^{-1}A - I$, which the authors assume to be of numerically low-rank if the factorization is sufficiently accurate. Apart from achieving faster convergence on a number of test-matrices, their framework extends to incorporate both incomplete LU (ILU) \cite{saad_iterative_2003} as well as block low-rank (BLR) LU \cite{amestoy_improving_2015} in addition to low-precision LU factorization. This enabled the algorithm to be applicable to a larger set of matrices, for which a factorization cost of $\mathcal{O}(n^3)$ is still infeasible, even if calculated in lower precision. However, despite working well on the evaluated matrices, the numerical behaviour of the preconditioner (with regards to the initial low-rank assumption) on large-scale problems remains to be verified in further research.



