\section{Background}
\label{sec:background}

The most general description of a (non)linear system can be given by a table of all possible inputs and the possible results (outputs) for those inputs \cite{kailath_linear_1980}, illustrating all transformations conducted by the systems. A more specific denotation of a linear system is given by $Ax=b$, where $A$ is a square, non-singular matrix of dimensions $n \times n$ and $b$ (the output) is a given column vector of $n$ rows. One well-known method to determine the corresponding solution $x$ (another column vector of $n$ rows) for such a system is Gaussian elimination \cite{greenbaum_iterative_1997}. The key idea is to transform $A$ into an upper triangular matrix $U$, by introducing zeros below the diagonal, one column at a time. This process is equivalent to multiplying $A$ a with a lower triangular matrix $L^{-1}$ from the left, thus giving the factorization $A=LU$ \cite{trefethen_numerical_1997}. Using this factorization, the system can then be solved by forward/backward-substitution using $La=b$ and $Ux=a$, with the factorization taking about $n^3/3$ operations, while the triangular solves require approximately $n^2/2$ multiply-subtracts each \cite{strang_introduction_2009}.

Consequently, the main disadvantage of this algorithm is that the runtime is of order $\mathcal{O}(n^3)$, even though the input data only involves $\mathcal{O}(n^2)$ numbers. 
Depending on the underlying structure of the matrix $A$, various methods have been developed in order to obtain a faster solution, one of the most popular being iterative methods  \cite{strang_introduction_2009}. Following the description by \cite{trefethen_numerical_1997}, a conventional factorization consists of $\mathcal{O}(n)$ steps, each of them requiring $\mathcal{O}(n^2)$ work. An ideal iterative method would be able to reduce the number of steps from $\mathcal{O}(n)$ to $\mathcal{O}(1)$ and the work per step from $\mathcal{O}(n^2)$ to $\mathcal{O}(n)$. Typical improvements are, however, to reduce the overall complexity from $\mathcal{O}(n^3)$ down to $\mathcal{O}(n^2)$ and in worst-case scenarios, when the matrix is very ill-conditioned, no improvements can be achieved at all.

In principle, an iterative method tries to construct a sequence of improving approximate solutions to the linear system, until it does not improve anymore or a desired accuracy is reached. Therefore, the runtime depends on the number of iterations required before convergence, which in turn depends on the spectral properties of the matrix $A$. In this context, a broad interpretation of the word "spectral" is used, for example, the conjugate gradient method is known to converge quickly, if the eigenvalues are clustered well away from the origin \cite{trefethen_numerical_1997}. As a consequence, iterative methods are only suitable if such a fast convergence can be achieved, but many linear systems do not meet those prerequisites. 

Fortunately, this can be alleviated by the introduction of a \textit{preconditioner}, basically trying to create a favourable environment for the iterative method to converge quickly. For a given linear system, this requires the design of  a matrix $M$, which, according to \cite{golub_matrix_2013} must have two basic properties:
\begin{itemize}
    \item The key features of $A$ must be reflected in the matrix by $M$
    \item It must be relatively easy to solve a linear system involving $M$, e.g. $Mz=r$
\end{itemize}
The key to successful preconditioning is then to find such a matrix $M$ that can be solved quickly, but still manages to be close enough to $A$ for the iterative method to converge fast enough \cite{trefethen_numerical_1997}. Furthermore, \cite{golub_matrix_2013} remarks that in the general case Krylov subspace methods (most practically used iterative methods belong to this category) are only successful, if an effective preconditioner is applied, making it an integral part of those algorithms.

However, even a well preconditioned iterative method will, in principle, only be able to deliver an approximation of the exact answer, regardless of the limitations of numerical computing (i.e. rounding errors). But, since even direct methods can not be calculated exactly on current computers, being accurate to a predefined precision level (or in the best case, machine precision) is usually considered to be good enough \cite{trefethen_numerical_1997}. In fact, if the matrix $A$ is very ill-conditioned, even the solution of direct factorization becomes highly inaccurate. For example, when the condition number $\kappa(A) = \left\| A \right\| \left\| A^{-1} \right\|$ is of order of the reciprocal of the unit round-off $u$ (i.e. $\kappa(A) = u^{-1}$), not a single accurate digit can be expected in the solution computed by a standard direct solving technique \cite{carson_new_2017}. The reason behind this deterioration of the result is simple: The rounding errors introduced by representing the system in finite machine precision get magnified by the condition number of the matrix $A$, leading to a larger error in the calculated solution $x$ \cite{buttari_mixed_2007}. 
This can be a serious restriction for applications where accurate solutions are required such as problems that arise in experimental mathematics \cite{sarra_radial_2011}, biology \cite{ma_solving_2015, ma_reliable_2017} or physics \cite{bailey_high-precision_2015}.
