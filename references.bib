
@mastersthesis{spalthoff_pg_hierarchical_2020,
	address = {Tokyo},
	title = {Hierarchical {Low} {Rank} {Approximation}. {Fast} {Solvers} for {Dense} {Systems} of {Linear} {Equations}.},
	school = {Tokyo Insitute of Technology},
	author = {{Spalthoff, P.G.}},
	year = {2020},
}

@article{amestoy_mixed_2021,
	title = {Mixed {Precision} {Low} {Rank} {Approximations} and their {Application} to {Block} {Low} {Rank} {LU} {Factorization}},
	abstract = {We introduce a novel approach to exploit mixed precision arithmetic for low-rank approximations. Our approach is based on the observation that singular vectors associated with small singular values can be stored in lower precisions while preserving high accuracy overall. We provide an explicit criterion to determine which level of precision is needed for each singular vector. We apply this approach to block low-rank (BLR) matrices, most of whose oﬀ-diagonal blocks have low rank. We propose a new BLR LU factorization algorithm that exploits the mixed precision representation of the blocks. We carry out the rounding error analysis of this algorithm and prove that the use of mixed precision arithmetic does not compromise the numerical stability of BLR LU factorization. Moreover our analysis determines which level of precision is needed for each ﬂoatingpoint operation (ﬂop), and therefore guides us towards an implementation that is both robust and eﬃcient. We evaluate the potential of this new algorithm on a range of matrices coming from real-life problems in industrial and academic applications. We show that a large fraction of the entries in the LU factors and ﬂops to perform the BLR LU factorization can be safely switched to lower precisions, leading to signiﬁcant reductions of the storage and ﬂop costs, of up to a factor three using fp64, fp32, and bﬂoat16 arithmetics.},
	language = {en},
	author = {Amestoy, Patrick and Boiteau, Olivier and Buttari, Alfredo and Gerest, Matthieu and Jézéquel, Fabienne},
	year = {2021},
	pages = {27},
}

@article{demmel_error_2006,
	title = {Error {Bounds} from {Extra}-{Precise} {Iterative} {Refinement}},
	volume = {32},
	issn = {0098-3500},
	url = {https://doi.org/10.1145/1141885.1141894},
	doi = {10.1145/1141885.1141894},
	abstract = {We present the design and testing of an algorithm for iterative refinement of the solution of linear equations where the residual is computed with extra precision. This algorithm was originally proposed in 1948 and analyzed in the 1960s as a means to compute very accurate solutions to all but the most ill-conditioned linear systems. However, two obstacles have until now prevented its adoption in standard subroutine libraries like LAPACK: (1) There was no standard way to access the higher precision arithmetic needed to compute residuals, and (2) it was unclear how to compute a reliable error bound for the computed solution. The completion of the new BLAS Technical Forum Standard has essentially removed the first obstacle. To overcome the second obstacle, we show how the application of iterative refinement can be used to compute an error bound in any norm at small cost and use this to compute both an error bound in the usual infinity norm, and a componentwise relative error bound.We report extensive test results on over 6.2 million matrices of dimensions 5, 10, 100, and 1000. As long as a normwise (componentwise) condition number computed by the algorithm is less than 1/max10,√nεw, the computed normwise (componentwise) error bound is at most 2 max10, √n · εw, and indeed bounds the true error. Here, n is the matrix dimension and εw = 2-24 is the working precision. Residuals were computed in double precision (53 bits of precision). In other words, the algorithm always computed a tiny error at negligible extra cost for most linear systems. For worse conditioned problems (which we can detect using condition estimation), we obtained small correct error bounds in over 90\% of cases.},
	number = {2},
	journal = {ACM Trans. Math. Softw.},
	author = {Demmel, James and Hida, Yozo and Kahan, William and Li, Xiaoye S. and Mukherjee, Sonil and Riedy, E. Jason},
	month = jun,
	year = {2006},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	keywords = {BLAS, LAPACK, Linear algebra, floating-point arithmetic},
	pages = {325--351},
}

@article{carson_accelerating_2018,
	title = {Accelerating the {Solution} of {Linear} {Systems} by {Iterative} {Refinement} in {Three} {Precisions}},
	volume = {40},
	issn = {1064-8275, 1095-7197},
	url = {https://epubs.siam.org/doi/10.1137/17M1140819},
	doi = {10.1137/17M1140819},
	abstract = {We propose a general algorithm for solving an n{\textbackslash}times n nonsingular linear system Ax = b based on iterative refinement with three precisions. The working precision is combined with possibly different precisions for solving for the correction term and for computing the residuals. Via rounding error analysis of the algorithm we derive sufficient conditions for convergence and bounds for the attainable forward error and normwise and componentwise backward errors. Our results generalize and unify many existing rounding error analyses for iterative refinement. With single precision as the working precision, we show that by using LU factorization in IEEE half precision as the solver and calculating the residuals in double precision it is possible to solve Ax = b to full single precision accuracy for {\textbackslash}infty -norm condition numbers {\textbackslash}kapa {\textbackslash}infty (A) {\textbackslash}leq  104, with the O(n3) part of the computations carried out entirely in half precision. We show further that by solving the correction equations by GMRES preconditioned by the LU factors the restriction on the condition number can be weakened to {\textbackslash}kapa {\textbackslash}infty (A) {\textbackslash}leq  108, although in general there is no guarantee that GMRES will converge quickly. Taking for comparison a standard Ax = b solver that uses LU factorization in single precision, these results suggest that on architectures for which half precision is efficiently implemented it will be possible to solve certain linear systems Ax = b up to twice as fast and to greater accuracy. Analogous results are given with double precision as the working precision.},
	language = {en},
	number = {2},
	urldate = {2021-06-29},
	journal = {SIAM Journal on Scientific Computing},
	author = {Carson, Erin and Higham, Nicholas J.},
	month = jan,
	year = {2018},
	pages = {A817--A847},
}

@article{ma_accuracy_2018,
	title = {Accuracy {Directly} {Controlled} {Fast} {Direct} {Solution} of {General} {H}{\textasciicircum}2-matrices and {Its} {Application} to {Solving} {Electrodynamics} {Volume} {Integral} {Equations}},
	volume = {66},
	doi = {10.1109/TMTT.2017.2734090},
	number = {1},
	journal = {IEEE Transactions on Microwave Theory and Techniques},
	author = {Ma, M. and Jiao, D.},
	year = {2018},
	note = {tex.ids= ma2018a
number: 1},
	keywords = {BEM, H2-matrix, LRA, ULV, electromagnetics, qsparse, volume integral},
	pages = {35--48},
}

@article{carratala-saez_exploiting_2019,
	title = {Exploiting nested task-parallelism in the \${\textbackslash}mathcal\{{H}\}-{LU}\$ factorization},
	volume = {33},
	issn = {18777503},
	url = {http://arxiv.org/abs/1906.00874},
	doi = {10.1016/j.jocs.2019.02.004},
	abstract = {We address the parallelization of the LU factorization of hierarchical matrices (\${\textbackslash}mathcal\{H\}\$-matrices) arising from boundary element methods. Our approach exploits task-parallelism via the OmpSs programming model and runtime, which discovers the data-flow parallelism intrinsic to the operation at execution time, via the analysis of data dependencies based on the memory addresses of the tasks' operands. This is especially challenging for \${\textbackslash}mathcal\{H\}\$-matrices, as the structures containing the data vary in dimension during the execution. We tackle this issue by decoupling the data structure from that used to detect dependencies. Furthermore, we leverage the support for weak operands and early release of dependencies, recently introduced in OmpSs-2, to accelerate the execution of parallel codes with nested task-parallelism and fine-grain tasks.},
	urldate = {2021-06-28},
	journal = {Journal of Computational Science},
	author = {Carratalá-Sáez, Rocío and Christophersen, Sven and Aliaga, José I. and Beltran, Vicenç and Börm, Steffen and Quintana-Ortí, Enrique S.},
	month = apr,
	year = {2019},
	note = {arXiv: 1906.00874},
	keywords = {68W10, 65N38, 65F05, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Mathematical Software, Mathematics - Numerical Analysis},
	pages = {20--33},
}

@article{carratala-saez_exploiting_2019-1,
	title = {Exploiting nested task-parallelism in the \${\textbackslash}mathcal\{{H}\}-{LU}\$ factorization},
	volume = {33},
	issn = {18777503},
	url = {http://arxiv.org/abs/1906.00874},
	doi = {10.1016/j.jocs.2019.02.004},
	abstract = {We address the parallelization of the LU factorization of hierarchical matrices (\${\textbackslash}mathcal\{H\}\$-matrices) arising from boundary element methods. Our approach exploits task-parallelism via the OmpSs programming model and runtime, which discovers the data-flow parallelism intrinsic to the operation at execution time, via the analysis of data dependencies based on the memory addresses of the tasks' operands. This is especially challenging for \${\textbackslash}mathcal\{H\}\$-matrices, as the structures containing the data vary in dimension during the execution. We tackle this issue by decoupling the data structure from that used to detect dependencies. Furthermore, we leverage the support for weak operands and early release of dependencies, recently introduced in OmpSs-2, to accelerate the execution of parallel codes with nested task-parallelism and fine-grain tasks.},
	urldate = {2021-06-28},
	journal = {Journal of Computational Science},
	author = {Carratalá-Sáez, Rocío and Christophersen, Sven and Aliaga, José I. and Beltran, Vicenç and Börm, Steffen and Quintana-Ortí, Enrique S.},
	month = apr,
	year = {2019},
	note = {arXiv: 1906.00874},
	keywords = {68W10, 65N38, 65F05, Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Mathematical Software, Mathematics - Numerical Analysis},
	pages = {20--33},
}

@article{ooi_effect_2020,
	title = {Effect of {Mixed} {Precision} {Computing} on {H}-{Matrix} {Vector} {Multiplication} in {BEM} {Analysis}},
	url = {http://arxiv.org/abs/1911.00093},
	doi = {10.1145/3368474.3368479},
	abstract = {Hierarchical Matrix (H-matrix) is an approximation technique which splits a target dense matrix into multiple submatrices, and where a selected portion of submatrices are low-rank approximated. The technique substantially reduces both time and space complexity of dense matrix vector multiplication, and hence has been applied to numerous practical problems. In this paper, we aim to accelerate the H-matrix vector multiplication by introducing mixed precision computing, where we employ both binary64 (FP64) and binary32 (FP32) arithmetic operations. We propose three methods to introduce mixed precision computing to H-matrix vector multiplication, and then evaluate them in a boundary element method (BEM) analysis. The numerical tests examine the effects of mixed precision computing, particularly on the required simulation time and rate of convergence of the iterative (BiCG-STAB) linear solver. We confirm the effectiveness of the proposed methods.},
	urldate = {2021-04-06},
	journal = {Proceedings of the International Conference on High Performance Computing in Asia-Pacific Region},
	author = {Ooi, Rise and Iwashita, Takeshi and Fukaya, Takeshi and Ida, Akihiro and Yokota, Rio},
	month = jan,
	year = {2020},
	note = {arXiv: 1911.00093},
	keywords = {Computer Science - Distributed, Parallel, and Cluster Computing, Computer Science - Mathematical Software, mixed precision},
	pages = {92--101},
}

@article{borm_h2-matrix_2006,
	title = {\${H}{\textasciicircum}2\$-{Matrix} {Arithmetics} in {Linear} {Complexity}},
	volume = {77},
	url = {http://search.proquest.com/openview/7652d5a0ccaaccd1ac5bc10f7db0c0f5/1?pq-origsite=gscholar},
	journal = {Computing},
	author = {Börm, S.},
	year = {2006},
	keywords = {H2-matrix, LRA},
	pages = {1--28},
}

@article{chandrasekaran_fast_2006,
	title = {A {Fast} {ULV} {Decomposition} {Solver} for {Hierarchically} {Semiseparable} {Representations}},
	volume = {28},
	doi = {10.1137/S0895479803436652},
	number = {3},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	author = {Chandrasekaran, S. and Gu, M. and Pals, T.},
	year = {2006},
	note = {Number: 3},
	keywords = {HSS, LRA, ULV},
	pages = {603--622},
}

@article{martinsson_fast_2011,
	title = {A {Fast} {Randomized} {Algorithm} for {Computing} a {Hierarchically} {Semiseparable} {Representation} of a {Matrix}},
	volume = {32},
	issn = {0895-4798, 1095-7162},
	url = {http://epubs.siam.org/doi/10.1137/100786617},
	doi = {10.1137/100786617},
	abstract = {Randomized sampling has recently been proven a highly eﬃcient technique for computing approximate factorizations of matrices that have low numerical rank. This paper describes an extension of such techniques to a wider class of matrices that are not themselves rank-deﬁcient, but have oﬀ-diagonal blocks that are; speciﬁcally, the class of so called Hierarchically Semi-Separable (HSS) matrices. HSS matrices arise frequently in numerical analysis and signal processing, in particular in the construction of fast methods for solving diﬀerential and integral equations numerically. The HSS structure admits algebraic operations (matrix-vector multiplications, matrix factorizations, matrix inversion, etc.) to be performed very rapidly; but only once the HSS representation of the matrix has been constructed. How to rapidly compute this representation in the ﬁrst place is much less well understood. The present paper demonstrates that if an N × N matrix can be applied to a vector in O(N ) time, and if individual entries of the matrix can be computed rapidly, then provided that an HSS representation of the matrix exists, it can be constructed in O(N k2) operations, where k is an upper bound for the numerical rank of the oﬀ-diagonal blocks. The point is that when legacy codes (based on, e.g., the Fast Multipole Method) can be used for the fast matrix-vector multiply, the proposed algorithm can be used to obtain the HSS representation of the matrix, and then well-established techniques for HSS matrices can be used to invert or factor the matrix.},
	language = {en},
	number = {4},
	urldate = {2021-03-03},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	author = {Martinsson, P. G.},
	month = oct,
	year = {2011},
	pages = {1251--1274},
}

@article{xia_fast_2010,
	title = {Fast {Algorithms} for {Hierarchically} {Semiseperable} {Matrices}},
	volume = {17},
	journal = {Numerical Linear Algebra with Applications},
	author = {Xia, J. and Chandrasekaran, S. and Gu, M. and Li, X. S.},
	year = {2010},
	keywords = {HSS, LRA, ULV},
	pages = {953--976},
}

@article{ambikasaran_fast_2016,
	title = {Fast {Direct} {Methods} for {Gaussian} {Processes}},
	volume = {38},
	number = {2},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Ambikasaran, S. and Foreman-Mackey, D. and Greengard, L. and Hogg, D. W. and O'Neil, M.},
	year = {2016},
	note = {Number: 2},
	keywords = {Gaussian process, HODLR, LRA},
	pages = {252--265},
}

@article{ambikasaran_hodlrlib_2019,
	title = {{HODLRlib}: {A} {Library} for {Hierarchical} {Matrices}},
	doi = {10.21105/joss.01167},
	journal = {The Journal of Open Source Software},
	author = {Ambikasaran, S. and Singh, K. R. and Sankaran, S. S.},
	year = {2019},
	keywords = {HODLR, LRA},
}

@article{hong_rank-revealing_1992,
	title = {Rank-{Revealing} {QR} {Factorizations} and the {Singular} {Value} {Decomposition}},
	volume = {58},
	issn = {00255718},
	url = {https://www.jstor.org/stable/2153029?origin=crossref},
	doi = {10.2307/2153029},
	abstract = {T. Chan has noted that, even when the singularvalue decomposition of a matrix A is known, it is still not obvious how to find a rank-revealing QR factorization(RRQR) of A if A has numericalrankdeficiency. This paper offersa constructiveproof of the existence of the RRQR factorizationof any matrix A of size m x n with numerical rank r. The bounds derived in this paper that guaranteethe existence of RRQR are all of order V/ir, in comparisonwith Chan's 0(2n-r) . It has been known for some time that if A is only numericallyrank-onedeficient,then the column permutation II of A that guaranteesa small rnn in the QR factorizationof Afl can be obtainedby inspectingthe size of the elementsof the rightsingularvectorof A corresponding to the smallestsingularvalue of A . To some extent, our papergeneralizes this well-knownresult.},
	language = {en},
	number = {197},
	urldate = {2021-06-22},
	journal = {Mathematics of Computation},
	author = {Hong, Y. P. and Pan, C.-T.},
	month = jan,
	year = {1992},
	pages = {213},
}

@article{gu_efficient_1996,
	title = {Efficient {Algorithms} for {Computing} a {Strong} {Rank}-{Revealing} {QR} {Factorization}},
	volume = {17},
	issn = {1064-8275, 1095-7197},
	url = {http://epubs.siam.org/doi/10.1137/0917055},
	doi = {10.1137/0917055},
	abstract = {Given an m n matrix M with m {\textgreater} n, it is shown that there exists a permutation FI and an integer k such that the QR factorization MYI= Q(Ak ckBk) reveals the numerical rank of M: the k k upper-triangular matrix Ak is well conditioned, IlCkll2 is small, and Bk is linearly dependent on Ak with coefficients bounded by a low-degree polynomial in n. Existing rank-revealing QR (RRQR) algorithms are related to such factorizations and two algorithms are presented for computing them. The new algorithms are nearly as efficient as QR with column pivoting for most problems and take O (ran2) floating-point operations in the worst case.},
	language = {en},
	number = {4},
	urldate = {2021-06-22},
	journal = {SIAM Journal on Scientific Computing},
	author = {Gu, Ming and Eisenstat, Stanley C.},
	month = jul,
	year = {1996},
	pages = {848--869},
}

@unpublished{ashcraft_block_2020,
	title = {Block {Low}-{Rank} {Matrices} with {Shared} {Bases}: {Potential} and {Limitations} of the {BLR}{\textasciicircum}2 {Format}},
	shorttitle = {Block {Low}-{Rank} {Matrices} with {Shared} {Bases}},
	url = {https://hal.archives-ouvertes.fr/hal-03070416},
	urldate = {2021-02-16},
	author = {Ashcraft, Cleve and Buttari, Alfredo and Mary, Théo},
	month = dec,
	year = {2020},
	keywords = {BLR, BLR2, Data sparse matrices, LRA, LU factorization, block low-rank matrices, block separable matrices, hierarchical matrices, numerical linear algebra, shared basis},
}

@article{hackbusch_h2-matrix_2002,
	title = {\${H}{\textasciicircum}2\$-{Matrix} {Approximation} of {Integral} {Operators} by {Interpolation}},
	volume = {43},
	doi = {10.1016/S0168-9274(02)00121-6},
	journal = {Applied Numerical Mathematics},
	author = {Hackbusch, W. and Börm, S.},
	year = {2002},
	keywords = {BEM, H2-matrix, LRA},
	pages = {129--143},
}

@article{amestoy_bridging_2019,
	title = {Bridging the {Gap} {Between} {Flat} and {Hierarchical} {Low}-{Rank} {Matrix} {Formats}: {The} {Multilevel} {Blr} {Format}},
	volume = {41},
	shorttitle = {Bridging the {Gap} {Between} {Flat} and {Hierarchical} {Low}-{Rank} {Matrix} {Formats}},
	url = {https://hal.archives-ouvertes.fr/hal-01774642},
	doi = {10.1137/18M1182760},
	abstract = {Matrices possessing a low-rank property arise in numerous scientific applications. This property can be exploited to provide a substantial reduction of the complexity of their LU or LDL T factorization. Among the possible low-rank formats, the flat Block Low-Rank (BLR) format is easy to use but achieves superlinear complexity. Alternatively, the hierarchical formats achieve linear complexity at the price of a much more complex, hierarchical matrix representation. In this paper, we propose a new format based on multilevel BLR approximations: the matrix is recursively defined as a BLR matrix whose full-rank blocks are themselves represented by BLR matrices. We call this format multilevel BLR (MBLR). Contrarily to hierarchical matrices, the number of levels in the block hierarchy is fixed to a given constant; while this format can still be represented within the H formalism, we show that applying the H theory to it leads to very pessimistic complexity bounds. We therefore extend the theory to prove better bounds, and show that the MBLR format provides a simple way to finely control the desired complexity of dense factorizations. By striking a balance between the simplicity of the BLR format and the low complexity of the hierarchical ones, the MBLR format bridges the gap between flat and hierarchical low-rank matrix formats. The MBLR format is of particular relevance in the context of sparse direct solvers, for which it is able to trade off the optimal dense complexity of the hierarchical formats to benefit from the simplicity and flexibility of the BLR format while still achieving O(n) sparse complexity. We finally compare our MBLR format with the related BLR-H (or Lattice-H) format; our theoretical analysis shows that both formats achieve the same asymptotic complexity for a given top level block size.},
	number = {3},
	urldate = {2021-02-16},
	journal = {SIAM Journal on Scientific Computing},
	author = {Amestoy, Patrick R. and Buttari, Alfredo and L'Excellent, Jean-Yves and Mary, Theo},
	month = may,
	year = {2019},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	keywords = {15a23, 65f05, 65f50, 65n30, 65y20, BLR, LRA, hierarchical matrices AMS subject classifications 15a06, low-rank approximations, matrix factorization, multilevel, sparse linear algebra},
	pages = {A1414--A1442},
}

@article{amestoy_improving_2015,
	title = {Improving {Multifrontal} {Methods} by {Means} of {Block} {Low}-{Rank} {Representations}},
	volume = {37},
	journal = {SIAM J. Sci. Comput.},
	author = {Amestoy, P. and Ashcraft, C. and Boiteau, O. and Buttari, A. and L’Excellent, J. and Weisbecker, Clément},
	year = {2015},
}

@article{amestoy_complexity_2017,
	title = {On the {Complexity} of the {Block} {Low}-{Rank} {Multifrontal} {Factorization}},
	volume = {39},
	doi = {10.1137/16M1077192},
	journal = {SIAM Journal on Scientific Computing},
	author = {Amestoy, Patrick and Buttari, Alfredo and L'Excellent, Jean-Yves and Mary, Theo},
	year = {2017},
	pages = {A1710--A1740},
}

@article{hackbusch_hierarchical_2004,
	title = {Hierarchical {Matrices} {Based} on a {Weak} {Admissibility} {Criterion}},
	volume = {73},
	doi = {10.1007/s00607-004-0080-4},
	journal = {Computing},
	author = {Hackbusch, Wolfgang and Khoromskij, B. and Kriemann, R.},
	year = {2004},
	pages = {207--243},
}

@book{hackbusch_hierarchical_2015,
	title = {Hierarchical {Matrices}: {Algorithms} and {Analysis}},
	volume = {49},
	isbn = {978-3-662-47323-8},
	publisher = {Springer Publishing Company, Incorporated},
	author = {Hackbusch, Wolfgang},
	year = {2015},
	doi = {10.1007/978-3-662-47324-5},
}

@book{bebendorf_hierarchical_2008,
	edition = {1st},
	title = {Hierarchical {Matrices}: {A} {Means} to {Efficiently} {Solve} {Elliptic} {Boundary} {Value} {Problems}},
	isbn = {3-540-77146-8},
	abstract = {Hierarchical matrices are an efficient framework for large-scale fully populated matrices arising, e.g., from the finite element discretization of solution operators of elliptic boundary value problems. In addition to storing such matrices, approximations of the usual matrix operations can be computed with logarithmic-linear complexity, which can be exploited to setup approximate preconditioners in an efficient and convenient way. Besides the algorithmic aspects of hierarchical matrices, the main aim of this book is to present their theoretical background. The book contains the existing approximation theory for elliptic problems including partial differential operators with nonsmooth coefficients. Furthermore, it presents in full detail the adaptive cross approximation method for the efficient treatment of integral operators with non-local kernel functions. The theory is supported by many numerical experiments from real applications.},
	publisher = {Springer Publishing Company, Incorporated},
	author = {Bebendorf, Mario},
	year = {2008},
}

@article{rigal_compatibility_1967,
	title = {On the {Compatibility} of a {Given} {Solution} {With} the {Data} of a {Linear} {System}},
	volume = {14},
	issn = {0004-5411},
	url = {https://doi.org/10.1145/321406.321416},
	doi = {10.1145/321406.321416},
	abstract = {Some new theorems generalizing a result of Oettli and Prager are applied to the a posteriori analysis of the compatibility of a computed solution to the uncertain data of a linear system (or of a polynomial equation).},
	number = {3},
	journal = {J. ACM},
	author = {Rigal, J. L. and Gaches, J.},
	month = jul,
	year = {1967},
	note = {Place: New York, NY, USA
Publisher: Association for Computing Machinery},
	pages = {543--548},
}

@article{saad_flexible_1993,
	title = {A {Flexible} {Inner}-{Outer} {Preconditioned} {GMRES} {Algorithm}},
	volume = {14},
	journal = {SIAM J. Sci. Comput.},
	author = {Saad, Y.},
	year = {1993},
	pages = {461--469},
}

@inproceedings{rjasanow_adaptive_2000,
	title = {Adaptive {Cross} {Approximation} of {Dense} {Matrices}},
	booktitle = {Internatinal {Association} for {Boundary} {Element} {Methods} {Conference}, {IABEM}},
	author = {Rjasanow, S.},
	year = {2000},
}

@article{gatto_efficient_2017,
	title = {Efficient {Preconditioning} of {Hp}-{FEM} {Matrices} by {Hierarchical} {Low}-{Rank} {Approximations}},
	volume = {72},
	issn = {0885-7474},
	url = {https://doi.org/10.1007/s10915-016-0347-x},
	doi = {10.1007/s10915-016-0347-x},
	abstract = {We introduce a preconditioner based on a hierarchical low-rank compression scheme of Schur complements. The construction is inspired by standard nested dissection, and relies on the assumption that the Schur complements can be approximated, to high precision, by Hierarchically-Semi-Separable matrices. We build the preconditioner as an approximate \$\$LDM{\textasciicircum}t\$\$LDMt factorization of a given matrix A, and no knowledge of A in assembled form is required by the construction. The \$\$LDM{\textasciicircum}t\$\$LDMt factorization is amenable to fast inversion, and the action of the inverse can be determined fast as well. We investigate the behavior of the preconditioner in the context of DG finite element approximations of elliptic and hyperbolic problems, with respect to both the mesh size and the order of approximation.},
	number = {1},
	journal = {J. Sci. Comput.},
	author = {Gatto, P. and Hesthaven, J. S.},
	month = jul,
	year = {2017},
	note = {Place: USA
Publisher: Plenum Press},
	keywords = {Indefinite operators, Interpolative decomposition, Preconditioned GMRES},
	pages = {49--80},
}

@article{gatto_preconditioner_2015,
	title = {A {Preconditioner} {Based} on {Low}-{Rank} {Approximation} of {Schur} {Complements}},
	url = {http://arxiv.org/abs/1508.07798},
	abstract = {We introduce a preconditioner based on a hierarchical low-rank compression scheme of Schur complements. The construction is inspired by standard nested dissection, and relies on the assumption that the Schur complements can be approximated, to high precision, by Hierarchically-Semi-Separable matrices. We build the preconditioner as an approximate \$LDM{\textasciicircum}t\$ factorization of a given matrix \$A\$, and no knowledge of \$A\$ in assembled form is required by the construction. The \$LDM{\textasciicircum}t\$ factorization is amenable to fast inversion, and the action of the inverse can be determined fast as well. We investigate the behavior of the preconditioner in the context of DG finite element approximations of elliptic and hyperbolic problems.},
	urldate = {2021-06-16},
	journal = {arXiv:1508.07798 [math]},
	author = {Gatto, Paolo and Hesthaven, Jan S.},
	month = aug,
	year = {2015},
	note = {arXiv: 1508.07798},
	keywords = {65F08, 65F50, Mathematics - Numerical Analysis},
}

@article{martinsson_randomized_2019,
	title = {Randomized methods for matrix computations},
	url = {http://arxiv.org/abs/1607.01649},
	abstract = {The purpose of this text is to provide an accessible introduction to a set of recently developed algorithms for factorizing matrices. These new algorithms attain high practical speed by reducing the dimensionality of intermediate computations using randomized projections. The algorithms are particularly powerful for computing low-rank approximations to very large matrices, but they can also be used to accelerate algorithms for computing full factorizations of matrices. A key competitive advantage of the algorithms described is that they require less communication than traditional deterministic methods.},
	language = {en},
	urldate = {2021-06-16},
	journal = {arXiv:1607.01649 [math]},
	author = {Martinsson, Per-Gunnar},
	month = feb,
	year = {2019},
	note = {arXiv: 1607.01649},
	keywords = {Mathematics - Numerical Analysis},
}

@article{eckart_approximation_1936,
	title = {The approximation of one matrix by another of lower rank},
	volume = {1},
	journal = {Psychometrika},
	author = {Eckart, C. and Young, G.},
	year = {1936},
	pages = {211--218},
}

@book{markovsky_low_2011,
	title = {Low {Rank} {Approximation}: {Algorithms}, {Implementation}, {Applications}},
	isbn = {1-4471-2226-7},
	abstract = {Data Approximation by Low-complexity Models details the theory, algorithms, and applications of structured low-rank approximation. Efficient local optimization methods and effective suboptimal convex relaxations for Toeplitz, Hankel, and Sylvester structured problems are presented. Much of the text is devoted to describing the applications of the theory including: system and control theory; signal processing; computer algebra for approximate factorization and common divisor computation; computer vision for image deblurring and segmentation; machine learning for information retrieval and clustering; bioinformatics for microarray data analysis; chemometrics for multivariate calibration; and psychometrics for factor analysis. Software implementation of the methods is given, making the theory directly applicable in practice. All numerical examples are included in demonstration files giving hands-on experience and exercises and MATLAB examples assist in the assimilation of the theory.},
	publisher = {Springer Publishing Company, Incorporated},
	author = {Markovsky, Ivan},
	year = {2011},
}

@article{sonneveld_cgs_1989,
	title = {{CGS}, {A} {Fast} {Lanczos}-{Type} {Solver} for {Nonsymmetric} {Linear} systems},
	volume = {10},
	journal = {Siam Journal on Scientific and Statistical Computing},
	author = {Sonneveld, P.},
	year = {1989},
	pages = {36--52},
}

@inproceedings{freund_transpose-free_1994,
	address = {New York, NY},
	title = {Transpose-{Free} {Quasi}-{Minimal} {Residual} {Methods} for {Non}-{Hermitian} {Linear} {Systems}},
	isbn = {978-1-4613-9353-5},
	abstract = {Recently, Freund and Nachtigal proposed a novel conjugate gradient-type method, the quasi-minimal residual algorithm (QMR), for the iterative solution of general non-Hermitian systems of linear equations. The QMR method is based on the nonsymmetric Lanczos process, and thus, like the latter, QMR requires matrix-vector multiplications with both the coefficient matrix of the linear system and its transpose. However, in certain applications, the transpose is not readily available, and generally, it is desirable to trade in multiplications with the transpose for matrix-vector products with the original matrix.},
	booktitle = {Recent {Advances} in {Iterative} {Methods}},
	publisher = {Springer New York},
	author = {Freund, Roland W.},
	editor = {Golub, Gene and Luskin, Mitchell and Greenbaum, Anne},
	year = {1994},
	pages = {69--94},
}

@article{van_der_vorst_bi-cgstab_1992,
	title = {Bi-{CGSTAB}: {A} {Fast} and {Smoothly} {Converging} {Variant} of {Bi}-{CG} for the {Solution} of {Nonsymmetric} {Linear} {Systems}},
	volume = {13},
	issn = {0196-5204},
	url = {https://doi.org/10.1137/0913035},
	doi = {10.1137/0913035},
	abstract = {Recently the Conjugate Gradients-Squared (CG-S) method has been proposed as an attractive variant of the Bi-Conjugate Gradients (Bi-CG) method. However, it has been observed that CG-S may lead to a rather irregular convergence behaviour, so that in some cases rounding errors can even result in severe cancellation effects in the solution. In this paper, another variant of Bi-CG is proposed which does not seem to suffer from these negative effects. Numerical experiments indicate also that the new variant, named Bi-CGSTAB, is often much more efficient than CG-S.},
	number = {2},
	journal = {SIAM J. Sci. Stat. Comput.},
	author = {van der Vorst, H. A.},
	month = mar,
	year = {1992},
	note = {Place: USA
Publisher: Society for Industrial and Applied Mathematics},
	keywords = {65F10, Bi-CG, CG-S, iterative solver, nonsymmetric linear systems, preconditioning},
	pages = {631--644},
}

@article{freund_qmr_1991,
	title = {{QMR}: a {Quasi}-{Minimal} {Residual} {Method} for {Non}-{Hermitian} {Linear} {Systems}},
	volume = {60},
	number = {1},
	journal = {Numerische Mathematik},
	author = {Freund, Roland W. and Nachtigal, Noël M.},
	year = {1991},
	pages = {315--339},
}

@article{paige_solution_1975,
	title = {Solution of {Sparse} {Indefinite} {Systems} of {Linear} {Equations}},
	volume = {12},
	url = {https://doi.org/10.1137/0712047},
	doi = {10.1137/0712047},
	number = {4},
	journal = {SIAM Journal on Numerical Analysis},
	author = {Paige, C. C. and Saunders, M. A.},
	year = {1975},
	note = {\_eprint: https://doi.org/10.1137/0712047},
	pages = {617--629},
}

@incollection{parlett_beresford_n_13_1998,
	address = {Philadelphia},
	title = {13. {Lanczos} {Algorithms}},
	isbn = {37675815},
	url = {https://epubs.siam.org/doi/abs/10.1137/1.9781611971163.ch13},
	booktitle = {The {Symmetric} {Eigenvalue} {Problem}},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {{Parlett, Beresford N.}},
	year = {1998},
	doi = {10.1137/1.9781611971163.ch13},
	note = {\_eprint: https://epubs.siam.org/doi/pdf/10.1137/1.9781611971163.ch13},
	pages = {287--321},
}

@article{chin_cg_2012,
	title = {{CG} versus {MINRES}: {An} empirical comparison},
	volume = {17},
	doi = {10.24200/squjs.vol17iss1pp44-62},
	journal = {Sultan Qaboos University Journal for Science [SQUJS]},
	author = {Chin, D. and Fong, L. and Saunders, Michael},
	year = {2012},
	pages = {44--62},
}

@article{hestenes_methods_1952,
	title = {Methods of conjugate gradients for solving linear systems},
	volume = {49},
	journal = {Journal of research of the National Bureau of Standards},
	author = {Hestenes, M. R. and Stiefel, E.},
	year = {1952},
	keywords = {imported},
	pages = {409--436},
}

@article{shewchuk_introduction_1994,
	title = {An {Introduction} to the {Conjugate} {Gradient} {Method} {Without} the {Agonizing} {Pain}},
	url = {http://www.cs.cmu.edu/ quake-papers/painless-conjugate-gradient.pdf},
	abstract = {The Conjugate Gradient Method is the most prominent iterative method for solving sparse systems of linear equations. Unfortunately, many textbook treatments of the topic are written with neither illustrations nor intuition, and their victims can be found to this day babbling senselessly in the corners of dusty libraries. For this reason, a deep, geometric understanding of the method has been reserved for the elite brilliant few who have painstakingly decoded the mumblings of their forebears. Nevertheless, the Conjugate Gradient Method is a composite of simple, elegant ideas that almost anyone can understand. Of course, a reader as intelligent as yourself will learn them almost effortlessly. The idea of quadratic forms is introduced and used to derive the methods of Steepest Descent, Conjugate Directions, and Conjugate Gradients. Eigenvectors are explained and used to examine the convergence of the Jacobi Method, Steepest Descent, and Conjugate Gradients. Other topics include preconditioning and the nonlinear Conjugate Gradient Method. I have taken pains to make this article easy to read. Sixty-two illustrations are provided. Dense prose is avoided. Concepts are explained in several different ways. Most equations are coupled with an intuitive interpretation.},
	author = {Shewchuk, Jonathan Richard},
	month = aug,
	year = {1994},
	keywords = {conjugate\_gradient\_method mathematics numerics},
}

@article{young_convergence_1970,
	title = {Convergence {Properties} of the {Symmetric} and {Unsymmetric} {Successive} {Overrelaxation} {Methods} and {Related} {Methods}},
	volume = {24},
	doi = {10.1090/S0025-5718-1970-0281331-4},
	abstract = {The paper is concerned with variants of the successive overrelaxation method (SOR method) for solving the linear system Au = b. Necessary and sufficient conditions are given for the convergence of the symmetric and unsymmetric SOR methods when A is symmetric. The modified SOR, symmetric SOR, and unsymmetric SOR methods are also considered for systems of the form Diu, - CuU2 = bi, - CLU1 + D2u2 = b2 where DI and D2 are square diagonal matrices. Different values of the relaxation factor are used on each set of equations. It is shown that if the matrix corresponding to the Jacobi method of iteration has real eigenvalues and has spectral radius ji {\textless} 1, then the spectral radius of the matrix G associated with any of the methods is not less than that of the ordinary SOR method with w = 2(1 + (1 - TA2)112)-l. Moreover, if the eigenvalues of G are real then no improvement is possible by the use of semi-iterative methods. Introduction. In this paper we study convergence properties of several iterative methods for solving the linear system (1.1) Au- b, where A is a given real nonsingular N X N matrix with nonvanishing diagonal elements, b is a given column vector, and u is a column vector to be determined. Each method can be characterized by an equation},
	journal = {Mathematics of Computation},
	author = {Young, D.},
	year = {1970},
	pages = {793--807},
}

@article{venit_convergence_1975,
	title = {The {Convergence} of {Jacobi} and {Gauss}-{Seidel} {Iteration}},
	volume = {48},
	issn = {0025-570X},
	url = {https://www.jstor.org/stable/2689699},
	doi = {10.2307/2689699},
	number = {3},
	urldate = {2021-06-08},
	journal = {Mathematics Magazine},
	author = {Venit, Stewart},
	year = {1975},
	note = {Publisher: Mathematical Association of America},
	pages = {163--167},
}

@article{paige_modified_2006,
	title = {Modified {Gram}-{Schmidt} ({MGS}), {Least} {Squares}, and {Backward} {Stability} of {MGS}-{GMRES}},
	volume = {28},
	issn = {0895-4798, 1095-7162},
	url = {http://epubs.siam.org/doi/10.1137/050630416},
	doi = {10.1137/050630416},
	abstract = {The generalized minimum residual method (GMRES) [Y. Saad and M. Schultz, SIAM J. Sci. Statist. Comput., 7 (1986), pp. 856–869] for solving linear systems Ax = b is implemented as a sequence of least squares problems involving Krylov subspaces of increasing dimensions. The most usual implementation is modiﬁed Gram–Schmidt GMRES (MGS-GMRES). Here we show that MGS-GMRES is backward stable. The result depends on a more general result on the backward stability of a variant of the MGS algorithm applied to solving a linear least squares problem, and uses other new results on MGS and its loss of orthogonality, together with an important but neglected condition number, and a relation between residual norms and certain singular values.},
	language = {en},
	number = {1},
	urldate = {2021-06-07},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	author = {Paige, Christopher C. and Rozlozník, Miroslav and Strakos, Zdenvek},
	month = jan,
	year = {2006},
	pages = {264--284},
}

@article{arnoldi_principle_1951,
	title = {The principle of minimized iterations in the solution of the matrix eigenvalue problem},
	volume = {9},
	journal = {Quarterly of Applied Mathematics},
	author = {Arnoldi, W.},
	year = {1951},
	pages = {17--29},
}

@book{singh_linear_2014,
	address = {Oxford, United Kingdom},
	title = {Linear algebra : step by step},
	publisher = {Oxford University Press},
	author = {Singh, Kuldeep},
	year = {2014},
}

@techreport{rump_approximate_1990,
	title = {Approximate inverses of almost singular matrices still contain useful information},
	copyright = {http://doku.b.tu-harburg.de/doku/lic\_ohne\_pod.php},
	url = {http://tubdok.tub.tuhh.de/handle/11420/321},
	abstract = {It is well-known that, roughly spoken, a matrix inversion on a computer working in base B with t digits precision in the mantissa applied to a matrix of condition Bk produces approximately t-k correct digits of the inverse. For condition {\textgreater}{\textgreater} Bt one might conclude that an approximate inverse contains virtually useless information. In this note we will show that the latter is not true. An approximate inverse may still be useful, e.g. as a preconditioner. An extended set of examples show that preconditioning a matrix using an approximate inverse (computed in t digits precision) lowers the condition number by a factor Bt. As an example we develop an algorithm for solving systems of linear equations up to condition B2t strictly using t digits precision for all calculations and only allowing for double precision accumulation of inner products.},
	institution = {Techn. Univ. Hamburg-Harburg},
	author = {Rump, Siegfried M.},
	year = {1990},
	doi = {10.15480/882.319},
	note = {Series: Berichte des Forschungsschwerpunktes Informations- und Kommunikationstechnik},
}

@book{kailath_linear_1980,
	address = {Englewood Cliffs, N.J., United States},
	title = {Linear systems},
	publisher = {Prentice-Hall},
	author = {Kailath, Thomas},
	year = {1980},
}

@article{arioli_using_2008,
	title = {Using {FGMRES} to obtain backward stability in mixed precision},
	volume = {33},
	journal = {Electronic Transactions on Numerical Analysis},
	author = {Arioli, Mario and Duff, Iain},
	year = {2008},
}

@article{institute_of_electrical_and_electronics_engineers_ieee_2008,
	title = {{IEEE} {Standard} for {Floating}-{Point} {Arithmetic}},
	doi = {10.1109/IEEESTD.2008.4610935},
	abstract = {This standard specifies interchange and arithmetic formats and methods for binary and decimal floating-point arithmetic in computer programming environments. This standard specifies exception conditions and their default handling. An implementation of a floating-point system conforming to this standard may be realized entirely in software, entirely in hardware, or in any combination of software and hardware. For operations specified in the normative part of this standard, numerical results and exceptions are uniquely determined by the values of the input data, sequence of operations, and destination formats, all under user control.},
	journal = {IEEE Std 754-2008},
	author = {{Institute of Electrical and Electronics Engineers}},
	month = aug,
	year = {2008},
	note = {Conference Name: IEEE Std 754-2008},
	keywords = {754-2008, Floating-point arithmetic, Hardware, IEEE Standards, Microprocessors, NaN, Software, Trademarks, arithmetic, binary, computer, decimal, exponent, floating-point, format, interchange, number, rounding, significand, subnormal},
	pages = {1--70},
}

@article{skeel_scaling_1979,
	title = {Scaling for {Numerical} {Stability} in {Gaussian} {Elimination}},
	volume = {26},
	issn = {0004-5411, 1557-735X},
	url = {https://dl.acm.org/doi/10.1145/322139.322148},
	doi = {10.1145/322139.322148},
	language = {en},
	number = {3},
	urldate = {2021-06-01},
	journal = {Journal of the ACM},
	author = {Skeel, Robert D.},
	month = jul,
	year = {1979},
	pages = {494--526},
}

@book{higham_accuracy_2002,
	address = {USA},
	edition = {2nd},
	title = {Accuracy and {Stability} of {Numerical} {Algorithms}},
	isbn = {0-89871-521-0},
	abstract = {From the Publisher:What is the most accurate way to sum floating point numbers\_\_ \_\_ What are the advantages of IEEE arithmetic\_\_ \_\_ How accurate is Gaussian elimination and what were the key breakthroughs in the development of error analysis for the method\_\_ \_\_ The answers to these and many related questions are included here. This book gives a thorough, up-to-date treatment of the behavior of numerical algorithms in finite precision arithmetic. It combines algorithmic derivations, perturbation theory, and rounding error analysis. Software practicalities are emphasized throughout, with particular reference to LAPACK and MATLAB. The best available error bounds, some of them new, are presented in a unified format with a minimum of jargon. Because of its central role in revealing problem sensitivity and providing error bounds, perturbation theory is treated in detail. Historical perspective and insight are given, with particular reference to the fundamental work of Wilkinson and Turing, and the many quotations provide further information in an accessible format. The book is unique in that algorithmic developments and motivations are given succinctly and implementation details minimized, so that attention can be concentrated on accuracy and stability results. Here, in one place and in a unified notation, is error analysis for most of the standard algorithms in matrix computations. Not since Wilkinson's Rounding Errors in Algebraic Processes (1963) and The Algebraic Eigenvalue Problem (1965) has any volume treated this subject in such depth. A number of topics are treated that are not usually covered in numerical analysis textbooks, including floating point summation, block LU factorization, condition number estimation, the Sylvester equation, powers of matrices, finite precision behavior of stationary iterative methods, Vandermonde systems, and fast matrix multiplication. Although not designed specifically as a textbook, this volume is a suitable reference for an advanced course, and could be used by instructors at all levels as a supplementary text from which to draw examples, historical perspective, statements of results, and exercises (many of which have never before appeared in textbooks). The book is designed to be a comprehensive reference and its bibliography contains more than 1100 references from the research literature. Audience Specialists in numerical analysis as well as computational scientists and engineers concerned about the accuracy of their results will benefit from this book. Much of the book can be understood with only a basic grounding in numerical analysis and linear algebra. About the Author Nicholas J. Higham is a Professor of Applied Mathematics at the University of Manchester, England. He is the author of more than 40 publications and is a member of the editorial boards of the SIAM Journal on Matrix Analysis and Applications and the IMA Journal of Numerical Analysis. His book Handbook of Writing for the Mathematical Sciences was published by SIAM in 1993.},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Higham, Nicholas J.},
	year = {2002},
}

@article{amestoy_improving_2015-1,
	title = {Improving {Multifrontal} {Methods} by {Means} of {Block} {Low}-{Rank} {Representations}},
	volume = {37},
	journal = {SIAM J. Sci. Comput.},
	author = {Amestoy, P. and Ashcraft, C. and Boiteau, O. and Buttari, A. and L’Excellent, J. and Weisbecker, Clément},
	year = {2015},
}

@book{saad_iterative_2003,
	address = {USA},
	edition = {2nd},
	title = {Iterative {Methods} for {Sparse} {Linear} {Systems}},
	isbn = {0-89871-534-2},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Saad, Y.},
	year = {2003},
}

@article{higham_new_2019,
	title = {A {New} {Preconditioner} that {Exploits} {Low}-{Rank} {Approximations} to {Factorization} {Error}},
	volume = {41},
	issn = {1064-8275, 1095-7197},
	url = {https://epubs.siam.org/doi/10.1137/18M1182802},
	doi = {10.1137/18M1182802},
	abstract = {We consider ill-conditioned linear systems Ax = b that are to be solved iteratively, and assume that a low accuracy LU factorization A ≈ LU is available for use in a preconditioner. We have observed that for ill-conditioned matrices A arising in practice, A−1 tends to be numerically low rank, that is, it has a small number of large singular values. Importantly, the error matrix E = U −1L−1A−I tends to have the same property. To understand this phenomenon we give bounds for the distance from E to a low-rank matrix in terms of the corresponding distance for A−1. We then design a novel preconditioner that exploits the low-rank property of the error to accelerate the convergence of iterative methods. We apply this new preconditioner in three diﬀerent contexts ﬁtting our general framework: low ﬂoating-point precision (e.g., half precision) LU factorization, incomplete LU factorization, and block low-rank LU factorization. In numerical experiments with GMRES-based iterative reﬁnement we show that our preconditioner can achieve a signiﬁcant reduction in the number of iterations required to solve a variety of real-life problems.},
	language = {en},
	number = {1},
	urldate = {2021-06-01},
	journal = {SIAM Journal on Scientific Computing},
	author = {Higham, Nicholas J. and Mary, Theo},
	month = jan,
	year = {2019},
	pages = {A59--A82},
}

@inproceedings{haidar_harnessing_2018,
	address = {Dallas, TX, USA},
	title = {Harnessing {GPU} {Tensor} {Cores} for {Fast} {FP16} {Arithmetic} to {Speed} up {Mixed}-{Precision} {Iterative} {Refinement} {Solvers}},
	isbn = {978-1-5386-8384-2},
	url = {https://ieeexplore.ieee.org/document/8665777/},
	doi = {10.1109/SC.2018.00050},
	abstract = {Low-precision floating-point arithmetic is a powerful tool for accelerating scientific computing applications, especially those in artificial intelligence. Here, we present an investigation showing that other high-performance computing (HPC) applications can also harness this power. Specifically, we use the general HPC problem, Ax = b, where A is a large dense matrix, and a double precision (FP64) solution is needed for accuracy. Our approach is based on mixed-precision (FP16---+FP64) iterative refinement, and we generalize and extend prior advances into a framework, for which we develop architecture-specific algorithms and highly tuned implementations. These new methods show how using half-precision Tensor Cores (FP16-TC) for the arithmetic can provide up to 4x speedup. This is due to the performance boost that the FP16-TC provide as well as to the improved accuracy over the classical FP16 arithmetic that is obtained because the GEMM accumulation occurs in FP32 arithmetic.},
	language = {en},
	urldate = {2021-06-01},
	booktitle = {{SC18}: {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	publisher = {IEEE},
	author = {Haidar, Azzam and Tomov, Stanimire and Dongarra, Jack and Higham, Nicholas J.},
	month = nov,
	year = {2018},
	pages = {603--613},
}

@article{carson_accelerating_2018-1,
	title = {Accelerating the {Solution} of {Linear} {Systems} by {Iterative} {Refinement} in {Three} {Precisions}},
	volume = {40},
	issn = {1064-8275, 1095-7197},
	url = {https://epubs.siam.org/doi/10.1137/17M1140819},
	doi = {10.1137/17M1140819},
	abstract = {We propose a general algorithm for solving an n{\textbackslash}times n nonsingular linear system Ax = b based on iterative refinement with three precisions. The working precision is combined with possibly different precisions for solving for the correction term and for computing the residuals. Via rounding error analysis of the algorithm we derive sufficient conditions for convergence and bounds for the attainable forward error and normwise and componentwise backward errors. Our results generalize and unify many existing rounding error analyses for iterative refinement. With single precision as the working precision, we show that by using LU factorization in IEEE half precision as the solver and calculating the residuals in double precision it is possible to solve Ax = b to full single precision accuracy for {\textbackslash}infty -norm condition numbers {\textbackslash}kapa {\textbackslash}infty (A) {\textbackslash}leq  104, with the O(n3) part of the computations carried out entirely in half precision. We show further that by solving the correction equations by GMRES preconditioned by the LU factors the restriction on the condition number can be weakened to {\textbackslash}kapa {\textbackslash}infty (A) {\textbackslash}leq  108, although in general there is no guarantee that GMRES will converge quickly. Taking for comparison a standard Ax = b solver that uses LU factorization in single precision, these results suggest that on architectures for which half precision is efficiently implemented it will be possible to solve certain linear systems Ax = b up to twice as fast and to greater accuracy. Analogous results are given with double precision as the working precision.},
	language = {en},
	number = {2},
	urldate = {2021-06-01},
	journal = {SIAM Journal on Scientific Computing},
	author = {Carson, Erin and Higham, Nicholas J.},
	month = jan,
	year = {2018},
	pages = {A817--A847},
}

@article{rump_inversion_2009,
	title = {Inversion of extremely {Ill}-conditioned matrices in floating-point},
	volume = {26},
	issn = {1868-937X},
	url = {https://doi.org/10.1007/BF03186534},
	doi = {10.1007/BF03186534},
	abstract = {Let ann xn matrixA of floating-point numbers in some format be given. Denote the relative rounding error unit of the given format by eps. AssumeA to be extremely ill-conditioned, that is cond(A) ≫ eps−1. In about 1984 I developed an algorithm to calculate an approximate inverse ofA solely using the given floating-point format. The key is a multiplicative correction rather than a Newton-type additive correction. I did not publish it because of lack of analysis. Recently, in [9] a modification of the algorithm was analyzed. The present paper has two purposes. The first is to present reasoning how and why the original algorithm works. The second is to discuss a quite unexpected feature of floating-point computations, namely, that an approximate inverse of an extraordinary ill-conditioned matrix still contains a lot of useful information. We will demonstrate this by inverting a matrix with condition number beyond 10300 solely using double precision. This is a workout of the invited talk at the SCAN meeting 2006 in Duisburg.},
	language = {en},
	number = {2},
	urldate = {2021-06-01},
	journal = {Japan Journal of Industrial and Applied Mathematics},
	author = {Rump, Siegfried M.},
	month = oct,
	year = {2009},
	pages = {249--277},
}

@article{saad_gmres_1986,
	title = {{GMRES}: {A} {Generalized} {Minimal} {Residual} {Algorithm} for {Solving} {Nonsymmetric} {Linear} {Systems}},
	volume = {7},
	issn = {0196-5204},
	shorttitle = {{GMRES}},
	url = {https://epubs.siam.org/doi/abs/10.1137/0907058},
	doi = {10.1137/0907058},
	abstract = {We present an iterative method for solving linear systems, which has the property of minimizing at every step the norm of the residual vector over a Krylov subspace. The algorithm is derived from the Arnoldi process for constructing an \$l\_2 \$-orthogonal basis of Krylov subspaces. It can be considered as a generalization of Paige and Saunders’ MINRES algorithm and is theoretically equivalent to the Generalized Conjugate Residual (GCR) method and to ORTHODIR. The new algorithm presents several advantages over GCR and ORTHODIR.},
	number = {3},
	urldate = {2021-05-28},
	journal = {SIAM Journal on Scientific and Statistical Computing},
	author = {Saad, Youcef and Schultz, Martin H.},
	month = jul,
	year = {1986},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {856--869},
}

@article{abdelfattah_linear_2016,
	title = {Linear algebra software for large-scale accelerated multicore computing*},
	volume = {25},
	issn = {0962-4929, 1474-0508},
	url = {https://www.cambridge.org/core/journals/acta-numerica/article/linear-algebra-software-for-largescale-accelerated-multicore-computing/D7EC58573BE22E2D6407DB0A330FC5FC},
	doi = {10.1017/S0962492916000015},
	abstract = {Many crucial scientific computing applications, ranging from national security to medical advances, rely on high-performance linear algebra algorithms and technologies, underscoring their importance and broad impact. Here we present the state-of-the-art design and implementation practices for the acceleration of the predominant linear algebra algorithms on large-scale accelerated multicore systems. Examples are given with fundamental dense linear algebra algorithms – from the LU, QR, Cholesky, and LDLT factorizations needed for solving linear systems of equations, to eigenvalue and singular value decomposition (SVD) problems. The implementations presented are readily available via the open-source PLASMA and MAGMA libraries, which represent the next generation modernization of the popular LAPACK library for accelerated multicore systems.To generate the extreme level of parallelism needed for the efficient use of these systems, algorithms of interest are redesigned and then split into well-chosen computational tasks. The task execution is scheduled over the computational components of a hybrid system of multicore CPUs with GPU accelerators and/or Xeon Phi coprocessors, using either static scheduling or light-weight runtime systems. The use of light-weight runtime systems keeps scheduling overheads low, similar to static scheduling, while enabling the expression of parallelism through sequential-like code. This simplifies the development effort and allows exploration of the unique strengths of the various hardware components. Finally, we emphasize the development of innovative linear algebra algorithms using three technologies – mixed precision arithmetic, batched operations, and asynchronous iterations – that are currently of high interest for accelerated multicore systems.},
	language = {en},
	urldate = {2021-05-28},
	journal = {Acta Numerica},
	author = {Abdelfattah, A. and Anzt, H. and Dongarra, J. and Gates, M. and Haidar, A. and Kurzak, J. and Luszczek, P. and Tomov, S. and Yamazaki, I. and YarKhan, A.},
	month = may,
	year = {2016},
	note = {Publisher: Cambridge University Press},
	pages = {1--160},
}

@inproceedings{langou_exploiting_2006,
	title = {Exploiting the {Performance} of 32 bit {Floating} {Point} {Arithmetic} in {Obtaining} 64 bit {Accuracy} ({Revisiting} {Iterative} {Refinement} for {Linear} {Systems})},
	doi = {10.1109/SC.2006.30},
	abstract = {Recent versions of microprocessors exhibit performance characteristics for 32 bit floating point arithmetic (single precision) that is substantially higher than 64 bit floating point arithmetic (double precision). Examples include the Intel's Pentium IV and M processors, AMD's Opteron architectures and the IBM's Cell Broad Engine processor. When working in single precision, floating point operations can be performed up to two times faster on the Pentium and up to ten times faster on the Cell over double precision. The performance enhancements in these architectures are derived by accessing extensions to the basic architecture, such as SSE2 in the case of the Pentium and the vector functions on the IBM Cell. The motivation for this paper is to exploit single precision operations whenever possible and resort to double precision at critical stages while attempting to provide the full double precision results. The results described here are fairly general and can be applied to various problems in linear algebra such as solving large sparse systems, using direct or iterative methods and some eigenvalue problems. There are limitations to the success of this process, such as when the conditioning of the problem exceeds the reciprocal of the accuracy of the single precision computations. In that case the double precision algorithm should be used},
	booktitle = {{SC} '06: {Proceedings} of the 2006 {ACM}/{IEEE} {Conference} on {Supercomputing}},
	author = {Langou, Julie and Langou, Julien and Luszczek, Piotr and Kurzak, Jakub and Buttari, Alfredo and Dongarra, Jack},
	month = nov,
	year = {2006},
	keywords = {Eigenvalues and eigenfunctions, Engines, Floating-point arithmetic, Iterative algorithms, Iterative methods, Lifting equipment, Linear algebra, Linear systems, Microprocessors, Permission},
	pages = {50--50},
}

@book{anderson_lapack_1999,
	address = {Philadelphia, PA},
	edition = {Third},
	title = {{LAPACK} {Users}' {Guide}},
	isbn = {0-89871-447-8 (paperback)},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Anderson, E. and Bai, Z. and Bischof, C. and Blackford, S. and Demmel, J. and Dongarra, J. and Du Croz, J. and Greenbaum, A. and Hammarling, S. and McKenney, A. and Sorensen, D.},
	year = {1999},
}

@article{skeel_iterative_1980,
	title = {Iterative {Refinement} {Implies} {Numerical} {Stability} for {Gaussian} {Elimination}},
	volume = {35},
	doi = {10.2307/2006197},
	journal = {Mathematics of Computation},
	author = {Skeel, Robert},
	year = {1980},
}

@article{higham_iterative_1997,
	title = {Iterative refinement for linear systems and {LAPACK}},
	volume = {17},
	issn = {0272-4979, 1464-3642},
	url = {https://academic.oup.com/imajna/article-lookup/doi/10.1093/imanum/17.4.495},
	doi = {10.1093/imanum/17.4.495},
	language = {en},
	number = {4},
	urldate = {2021-05-26},
	journal = {IMA Journal of Numerical Analysis},
	author = {Higham, N.},
	month = oct,
	year = {1997},
	pages = {495--509},
}

@article{higham_iterative_1991,
	title = {Iterative refinement enhances the stability {ofQR} factorization methods for solving linear equations},
	volume = {31},
	issn = {0006-3835, 1572-9125},
	url = {http://link.springer.com/10.1007/BF01933262},
	doi = {10.1007/BF01933262},
	abstract = {Iterative refinement is a well-known technique for improving the quality of an approximate solution to a linear system. In the traditional usage residuals are computed in extended precision, but more recent work has shown that fixed precision is sufficient to yield benefits for stability. We extend existing results to show that fixed precision iterative refinement renders an arbitrary linear equations solver backward stable in a strong, componentwise sense, under suitable assumptions. Two particular applications involving the QR factorization are discussed in detail: solution of square linear systems and solution of least squares problems. In the former case we show that one step of iterative refinement suffices to produce a small componentwise relative backward error. Our results are weaker for the least squares problem, but again we find that iterative refinement improves a componentwise measure of backward stability. In particular, iterative refinement mitigates the effect of poor row scaling of the coefficient matrix, and so provides an alternative to the use of row interchanges in the Householder QR factorization. A further application of the results is described to fast methods for solving Vandermondelike systems.},
	language = {en},
	number = {3},
	urldate = {2021-05-26},
	journal = {BIT},
	author = {Higham, Nicholas J.},
	month = sep,
	year = {1991},
	pages = {447--468},
}

@article{moler_iterative_1967,
	title = {Iterative {Refinement} in {Floating} {Point}},
	volume = {14},
	issn = {0004-5411, 1557-735X},
	url = {https://dl.acm.org/doi/10.1145/321386.321394},
	doi = {10.1145/321386.321394},
	abstract = {Iter{\textasciitilde}tive refinement reduces the roundoff errors in the computed solution to a system of linear equations. Only one step requires higher precision arithmetic. If sufficiently high precision is used, the final result is shown to be very accurate.},
	language = {en},
	number = {2},
	urldate = {2021-05-26},
	journal = {Journal of the ACM},
	author = {Moler, Cleve B.},
	month = apr,
	year = {1967},
	pages = {316--321},
}

@book{wilkinson_rounding_1963,
	title = {Rounding errors in algebraic processes},
	url = {http://archive.org/details/roundingerrorsin0000wilk},
	abstract = {vi, 161 p; Bibliography: p. 157-158},
	language = {eng},
	urldate = {2021-05-26},
	publisher = {Englewood Cliffs, N.J. : Prentice-Hall},
	author = {Wilkinson, J. H. (James Hardy)},
	collaborator = {{Internet Archive}},
	year = {1963},
	keywords = {Electronic digital computers},
}

@article{sarra_radial_2011,
	title = {Radial basis function approximation methods with extended precision floating point arithmetic},
	volume = {35},
	issn = {0955-7997},
	url = {https://www.sciencedirect.com/science/article/pii/S0955799710001256},
	doi = {10.1016/j.enganabound.2010.05.011},
	abstract = {Radial basis function (RBF) methods that employ infinitely differentiable basis functions featuring a shape parameter are theoretically spectrally accurate methods for scattered data interpolation and for solving partial differential equations. It is also theoretically known that RBF methods are most accurate when the linear systems associated with the methods are extremely ill-conditioned. This often prevents the RBF methods from realizing spectral accuracy in applications. In this work we examine how extended precision floating point arithmetic can be used to improve the accuracy of RBF methods in an efficient manner. RBF methods using extended precision are compared to algorithms that evaluate RBF methods by bypassing the solution of the ill-conditioned linear systems.},
	language = {en},
	number = {1},
	urldate = {2021-05-26},
	journal = {Engineering Analysis with Boundary Elements},
	author = {Sarra, Scott A.},
	month = jan,
	year = {2011},
	keywords = {Extended precision floating point arithmetic, RBF collocation for PDEs, RBF interpolation},
	pages = {68--76},
}

@article{ma_reliable_2017,
	title = {Reliable and efficient solution of genome-scale models of {Metabolism} and macromolecular {Expression}},
	volume = {7},
	copyright = {2017 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/srep40863},
	doi = {10.1038/srep40863},
	abstract = {Constraint-Based Reconstruction and Analysis (COBRA) is currently the only methodology that permits integrated modeling of Metabolism and macromolecular Expression (ME) at genome-scale. Linear optimization computes steady-state flux solutions to ME models, but flux values are spread over many orders of magnitude. Data values also have greatly varying magnitudes. Standard double-precision solvers may return inaccurate solutions or report that no solution exists. Exact simplex solvers based on rational arithmetic require a near-optimal warm start to be practical on large problems (current ME models have 70,000 constraints and variables and will grow larger). We have developed a quadruple-precision version of our linear and nonlinear optimizer MINOS, and a solution procedure (DQQ) involving Double and Quad MINOS that achieves reliability and efficiency for ME models and other challenging problems tested here. DQQ will enable extensive use of large linear and nonlinear models in systems biology and other applications involving multiscale data.},
	language = {en},
	number = {1},
	urldate = {2021-05-26},
	journal = {Scientific Reports},
	author = {Ma, Ding and Yang, Laurence and Fleming, Ronan M. T. and Thiele, Ines and Palsson, Bernhard O. and Saunders, Michael A.},
	month = jan,
	year = {2017},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {40863},
}

@inproceedings{ma_solving_2015,
	address = {Cham},
	series = {Springer {Proceedings} in {Mathematics} \& {Statistics}},
	title = {Solving {Multiscale} {Linear} {Programs} {Using} the {Simplex} {Method} in {Quadruple} {Precision}},
	isbn = {978-3-319-17689-5},
	doi = {10.1007/978-3-319-17689-5_9},
	abstract = {Systems biologists are developing increasingly large models of metabolism and integrated models of metabolism and macromolecular expression. These Metabolic Expression (ME) models lead to sequences of multiscale linear programs for which small solution values of order 10−6 to 10−10 are meaningful. Standard LP solvers do not give sufficiently accurate solutions, and exact simplex solvers are extremely slow. We investigate whether double-precision and quadruple-precision simplex solvers can together achieve reliability at acceptable cost.A double-precision LP solver often provides a reasonably good starting point for a Quad simplex solver. On a range of multiscale examples we find that 34-digit Quad floating-point achieves exceptionally small primal and dual infeasibilities (of order 10−30) when no more than 10−15 is requested. On a significant ME model we also observe robustness in almost all (even small) solution values following relative perturbations of order 10−6 to non-integer data values.Double and Quad Fortran 77 implementations of the linear and nonlinear optimization solver MINOS are available upon request.},
	language = {en},
	booktitle = {Numerical {Analysis} and {Optimization}},
	publisher = {Springer International Publishing},
	author = {Ma, Ding and Saunders, Michael A.},
	editor = {Al-Baali, Mehiddin and Grandinetti, Lucio and Purnama, Anton},
	year = {2015},
	keywords = {Flux balance analysis, Gfortran libquadmath, MINOS, Metabolic expression model, Multiscale linear program, Quadruple precision, Simplex method},
	pages = {223--235},
}

@article{bailey_high-precision_2015,
	title = {High-{Precision} {Arithmetic} in {Mathematical} {Physics}},
	volume = {3},
	doi = {10.3390/math3020337},
	abstract = {For many scientific calculations, particularly those involving empirical data, IEEE 32-bit floating-point arithmetic produces results of sufficient accuracy, while for other applications IEEE 64-bit floating-point is more appropriate. But for some very demanding applications, even higher levels of precision are often required. This article discusses the challenge of high-precision computation, in the context of mathematical physics, and highlights what facilities are required to support future computation, in light of emerging developments in computer architecture.},
	journal = {Mathematics},
	author = {Bailey, David and Borwein, Jonathan},
	month = may,
	year = {2015},
	pages = {337--367},
}

@book{golub_matrix_2013,
	edition = {Fourth},
	title = {Matrix {Computations}},
	isbn = {1-4214-0794-9 978-1-4214-0794-4},
	url = {http://www.cs.cornell.edu/cv/GVL4/golubandvanloan.htm},
	publisher = {JHU Press},
	author = {Golub, Gene H. and van Loan, Charles F.},
	year = {2013},
	keywords = {GvL cauchy circulant courant-fischer determinant dft eigenvalues interlacing linear.algebra matrix pseudoinverse textbook},
}

@book{trefethen_numerical_1997,
	title = {Numerical {Linear} {Algebra}},
	isbn = {0-89871-361-7},
	publisher = {SIAM},
	author = {Trefethen, Lloyd N. and Bau, David},
	year = {1997},
	keywords = {characteristic eigenvalues linear.algebra matrix numerical numerical.analysis polynomial secular.equation textbook},
}

@book{greenbaum_iterative_1997,
	address = {USA},
	title = {Iterative {Methods} for {Solving} {Linear} {Systems}},
	isbn = {0-89871-396-X},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Greenbaum, Anne},
	year = {1997},
}

@article{buttari_mixed_2007,
	title = {Mixed {Precision} {Iterative} {Refinement} {Techniques} for the {Solution} of {Dense} {Linear} {Systems}},
	volume = {21},
	issn = {1094-3420},
	url = {https://doi.org/10.1177/1094342007084026},
	doi = {10.1177/1094342007084026},
	abstract = {By using a combination of 32-bit and 64-bit floating point arithmetic, the performance of many dense and sparse linear algebra algorithms can be significantly enhanced while maintaining the 64-bit accuracy of the resulting solution. The approach presented here can apply not only to conventional processors but also to exotic technologies such as Field Programmable Gate Arrays (FPGA), Graphical Processing Units (GPU), and the Cell BE processor. Results on modern processor architectures and the Cell BE are presented.},
	language = {en},
	number = {4},
	urldate = {2021-05-21},
	journal = {The International Journal of High Performance Computing Applications},
	author = {Buttari, Alfredo and Dongarra, Jack and Langou, Julie and Langou, Julien and Luszczek, Piotr and Kurzak, Jakub},
	month = nov,
	year = {2007},
	note = {Publisher: SAGE Publications Ltd STM},
	keywords = {Cholesky factorization, LU, iterative, mixed-precision, refinement},
	pages = {457--466},
}

@book{davis_direct_2006,
	address = {USA},
	title = {Direct {Methods} for {Sparse} {Linear} {Systems} ({Fundamentals} of {Algorithms} 2)},
	isbn = {0-89871-613-6},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Davis, Timothy A.},
	year = {2006},
}

@article{carson_new_2017,
	title = {A {New} {Analysis} of {Iterative} {Refinement} and {Its} {Application} to {Accurate} {Solution} of {Ill}-{Conditioned} {Sparse} {Linear} {Systems}},
	volume = {39},
	issn = {1064-8275, 1095-7197},
	url = {https://epubs.siam.org/doi/10.1137/17M1122918},
	doi = {10.1137/17M1122918},
	abstract = {Iterative reﬁnement is a long-standing technique for improving the accuracy of a computed solution to a nonsingular linear system Ax = b obtained via LU factorization. It makes use of residuals computed in extra precision, typically at twice the working precision, and existing results guarantee convergence if the matrix A has condition number safely less than the reciprocal of the unit roundoﬀ, u. We identify a mechanism that allows iterative reﬁnement to produce solutions with normwise relative error of order u to systems with condition numbers of order u−1 or larger, provided that the update equation is solved with a relative error suﬃciently less than 1. A new rounding error analysis is given, and its implications are analyzed. Building on the analysis, we develop a GMRES (generalized minimal residual)-based iterative reﬁnement method (GMRES-IR) that makes use of the computed LU factors as preconditioners. GMRES-IR exploits the fact that even if A is extremely ill conditioned the LU factors contain enough information that preconditioning can greatly reduce the condition number of A. Our rounding error analysis and numerical experiments show that GMRES-IR can succeed where standard reﬁnement fails, and that it can provide accurate solutions to systems with condition numbers of order u−1 and greater. Indeed, in our experiments with such matrices—both random and from the University of Florida Sparse Matrix Collection—GMRES-IR yields a normwise relative error of order u in at most three steps in every case.},
	language = {en},
	number = {6},
	urldate = {2021-05-21},
	journal = {SIAM Journal on Scientific Computing},
	author = {Carson, Erin and Higham, Nicholas J.},
	month = jan,
	year = {2017},
	pages = {A2834--A2856},
}

@book{strang_introduction_2009,
	address = {Wellesley, MA},
	edition = {Fifth},
	title = {Introduction to {Linear} {Algebra}},
	isbn = {978-0-9802327-1-4 0-9802327-1-6 978-0-9802327-2-1 0-9802327-2-4 978-81-7596-811-0 81-7596-811-7},
	abstract = {Book Description: Gilbert Strang's textbooks have changed the entire approach to learning linear algebra – away from abstract vector spaces to specific examples of the four fundamental subspaces: the column space and nullspace of A and A'. Introduction to Linear Algebra, Fourth Edition includes challenge problems to complement the review problems that have been highly praised in previous editions. The basic course is followed by seven applications: differential equations, engineering, graph theory, statistics, Fourier methods and the FFT, linear programming, and computer graphics. Thousands of teachers in colleges and universities and now high schools are using this book, which truly explains this crucial subject.},
	publisher = {Wellesley-Cambridge Press},
	author = {Strang, Gilbert},
	year = {2009},
	keywords = {linear.algebra matrix strang textbook},
}
