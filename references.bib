
@article{saad_flexible_1993,
	title = {A {Flexible} {Inner}-{Outer} {Preconditioned} {GMRES} {Algorithm}},
	volume = {14},
	journal = {SIAM J. Sci. Comput.},
	author = {Saad, Y.},
	year = {1993},
	pages = {461--469},
}

@inproceedings{rjasanow_adaptive_2000,
	title = {Adaptive {Cross} {Approximation} of {Dense} {Matrices}},
	booktitle = {Internatinal {Association} for {Boundary} {Element} {Methods} {Conference}, {IABEM}},
	author = {Rjasanow, S.},
	year = {2000},
}

@article{gatto_efficient_2017,
	title = {Efficient {Preconditioning} of {Hp}-{FEM} {Matrices} by {Hierarchical} {Low}-{Rank} {Approximations}},
	volume = {72},
	issn = {0885-7474},
	url = {https://doi.org/10.1007/s10915-016-0347-x},
	doi = {10.1007/s10915-016-0347-x},
	abstract = {We introduce a preconditioner based on a hierarchical low-rank compression scheme of Schur complements. The construction is inspired by standard nested dissection, and relies on the assumption that the Schur complements can be approximated, to high precision, by Hierarchically-Semi-Separable matrices. We build the preconditioner as an approximate \$\$LDM{\textasciicircum}t\$\$LDMt factorization of a given matrix A, and no knowledge of A in assembled form is required by the construction. The \$\$LDM{\textasciicircum}t\$\$LDMt factorization is amenable to fast inversion, and the action of the inverse can be determined fast as well. We investigate the behavior of the preconditioner in the context of DG finite element approximations of elliptic and hyperbolic problems, with respect to both the mesh size and the order of approximation.},
	number = {1},
	journal = {J. Sci. Comput.},
	author = {Gatto, P. and Hesthaven, J. S.},
	month = jul,
	year = {2017},
	note = {Place: USA
Publisher: Plenum Press},
	keywords = {Indefinite operators, Interpolative decomposition, Preconditioned GMRES},
	pages = {49--80},
}

@article{gatto_preconditioner_2015,
	title = {A {Preconditioner} {Based} on {Low}-{Rank} {Approximation} of {Schur} {Complements}},
	url = {http://arxiv.org/abs/1508.07798},
	abstract = {We introduce a preconditioner based on a hierarchical low-rank compression scheme of Schur complements. The construction is inspired by standard nested dissection, and relies on the assumption that the Schur complements can be approximated, to high precision, by Hierarchically-Semi-Separable matrices. We build the preconditioner as an approximate \$LDM{\textasciicircum}t\$ factorization of a given matrix \$A\$, and no knowledge of \$A\$ in assembled form is required by the construction. The \$LDM{\textasciicircum}t\$ factorization is amenable to fast inversion, and the action of the inverse can be determined fast as well. We investigate the behavior of the preconditioner in the context of DG finite element approximations of elliptic and hyperbolic problems.},
	urldate = {2021-06-16},
	journal = {arXiv:1508.07798 [math]},
	author = {Gatto, Paolo and Hesthaven, Jan S.},
	month = aug,
	year = {2015},
	note = {arXiv: 1508.07798},
	keywords = {65F08, 65F50, Mathematics - Numerical Analysis},
}

@article{martinsson_randomized_2019,
	title = {Randomized methods for matrix computations},
	url = {http://arxiv.org/abs/1607.01649},
	abstract = {The purpose of this text is to provide an accessible introduction to a set of recently developed algorithms for factorizing matrices. These new algorithms attain high practical speed by reducing the dimensionality of intermediate computations using randomized projections. The algorithms are particularly powerful for computing low-rank approximations to very large matrices, but they can also be used to accelerate algorithms for computing full factorizations of matrices. A key competitive advantage of the algorithms described is that they require less communication than traditional deterministic methods.},
	language = {en},
	urldate = {2021-06-16},
	journal = {arXiv:1607.01649 [math]},
	author = {Martinsson, Per-Gunnar},
	month = feb,
	year = {2019},
	note = {arXiv: 1607.01649},
	keywords = {Mathematics - Numerical Analysis},
}

@article{eckart_approximation_1936,
	title = {The approximation of one matrix by another of lower rank},
	volume = {1},
	journal = {Psychometrika},
	author = {Eckart, C. and Young, G.},
	year = {1936},
	pages = {211--218},
}

@book{markovsky_low_2011,
	title = {Low {Rank} {Approximation}: {Algorithms}, {Implementation}, {Applications}},
	isbn = {1-4471-2226-7},
	abstract = {Data Approximation by Low-complexity Models details the theory, algorithms, and applications of structured low-rank approximation. Efficient local optimization methods and effective suboptimal convex relaxations for Toeplitz, Hankel, and Sylvester structured problems are presented. Much of the text is devoted to describing the applications of the theory including: system and control theory; signal processing; computer algebra for approximate factorization and common divisor computation; computer vision for image deblurring and segmentation; machine learning for information retrieval and clustering; bioinformatics for microarray data analysis; chemometrics for multivariate calibration; and psychometrics for factor analysis. Software implementation of the methods is given, making the theory directly applicable in practice. All numerical examples are included in demonstration files giving hands-on experience and exercises and MATLAB examples assist in the assimilation of the theory.},
	publisher = {Springer Publishing Company, Incorporated},
	author = {Markovsky, Ivan},
	year = {2011},
}

@article{sonneveld_cgs_1989,
	title = {{CGS}, {A} {Fast} {Lanczos}-{Type} {Solver} for {Nonsymmetric} {Linear} systems},
	volume = {10},
	journal = {Siam Journal on Scientific and Statistical Computing},
	author = {Sonneveld, P.},
	year = {1989},
	pages = {36--52},
}

@inproceedings{freund_transpose-free_1994,
	address = {New York, NY},
	title = {Transpose-{Free} {Quasi}-{Minimal} {Residual} {Methods} for {Non}-{Hermitian} {Linear} {Systems}},
	isbn = {978-1-4613-9353-5},
	abstract = {Recently, Freund and Nachtigal proposed a novel conjugate gradient-type method, the quasi-minimal residual algorithm (QMR), for the iterative solution of general non-Hermitian systems of linear equations. The QMR method is based on the nonsymmetric Lanczos process, and thus, like the latter, QMR requires matrix-vector multiplications with both the coefficient matrix of the linear system and its transpose. However, in certain applications, the transpose is not readily available, and generally, it is desirable to trade in multiplications with the transpose for matrix-vector products with the original matrix.},
	booktitle = {Recent {Advances} in {Iterative} {Methods}},
	publisher = {Springer New York},
	author = {Freund, Roland W.},
	editor = {Golub, Gene and Luskin, Mitchell and Greenbaum, Anne},
	year = {1994},
	pages = {69--94},
}

@article{van_der_vorst_bi-cgstab_1992,
	title = {Bi-{CGSTAB}: {A} {Fast} and {Smoothly} {Converging} {Variant} of {Bi}-{CG} for the {Solution} of {Nonsymmetric} {Linear} {Systems}},
	volume = {13},
	issn = {0196-5204},
	url = {https://doi.org/10.1137/0913035},
	doi = {10.1137/0913035},
	abstract = {Recently the Conjugate Gradients-Squared (CG-S) method has been proposed as an attractive variant of the Bi-Conjugate Gradients (Bi-CG) method. However, it has been observed that CG-S may lead to a rather irregular convergence behaviour, so that in some cases rounding errors can even result in severe cancellation effects in the solution. In this paper, another variant of Bi-CG is proposed which does not seem to suffer from these negative effects. Numerical experiments indicate also that the new variant, named Bi-CGSTAB, is often much more efficient than CG-S.},
	number = {2},
	journal = {SIAM J. Sci. Stat. Comput.},
	author = {van der Vorst, H. A.},
	month = mar,
	year = {1992},
	note = {Place: USA
Publisher: Society for Industrial and Applied Mathematics},
	keywords = {65F10, Bi-CG, CG-S, iterative solver, nonsymmetric linear systems, preconditioning},
	pages = {631--644},
}

@article{freund_qmr_1991,
	title = {{QMR}: a {Quasi}-{Minimal} {Residual} {Method} for {Non}-{Hermitian} {Linear} {Systems}},
	volume = {60},
	number = {1},
	journal = {Numerische Mathematik},
	author = {Freund, Roland W. and Nachtigal, NoÃ«l M.},
	year = {1991},
	pages = {315--339},
}

@article{paige_solution_1975,
	title = {Solution of {Sparse} {Indefinite} {Systems} of {Linear} {Equations}},
	volume = {12},
	url = {https://doi.org/10.1137/0712047},
	doi = {10.1137/0712047},
	number = {4},
	journal = {SIAM Journal on Numerical Analysis},
	author = {Paige, C. C. and Saunders, M. A.},
	year = {1975},
	note = {\_eprint: https://doi.org/10.1137/0712047},
	pages = {617--629},
}

@incollection{parlett_beresford_n_13_1998,
	address = {Philadelphia},
	title = {13. {Lanczos} {Algorithms}},
	isbn = {37675815},
	url = {https://epubs.siam.org/doi/abs/10.1137/1.9781611971163.ch13},
	booktitle = {The {Symmetric} {Eigenvalue} {Problem}},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {{Parlett, Beresford N.}},
	year = {1998},
	doi = {10.1137/1.9781611971163.ch13},
	note = {\_eprint: https://epubs.siam.org/doi/pdf/10.1137/1.9781611971163.ch13},
	pages = {287--321},
}

@article{chin_cg_2012,
	title = {{CG} versus {MINRES}: {An} empirical comparison},
	volume = {17},
	doi = {10.24200/squjs.vol17iss1pp44-62},
	journal = {Sultan Qaboos University Journal for Science [SQUJS]},
	author = {Chin, D. and Fong, L. and Saunders, Michael},
	year = {2012},
	pages = {44--62},
}

@article{hestenes_methods_1952,
	title = {Methods of conjugate gradients for solving linear systems},
	volume = {49},
	journal = {Journal of research of the National Bureau of Standards},
	author = {Hestenes, M. R. and Stiefel, E.},
	year = {1952},
	keywords = {imported},
	pages = {409--436},
}

@article{shewchuk_introduction_1994,
	title = {An {Introduction} to the {Conjugate} {Gradient} {Method} {Without} the {Agonizing} {Pain}},
	url = {http://www.cs.cmu.edu/ quake-papers/painless-conjugate-gradient.pdf},
	abstract = {The Conjugate Gradient Method is the most prominent iterative method for solving sparse systems of linear equations. Unfortunately, many textbook treatments of the topic are written with neither illustrations nor intuition, and their victims can be found to this day babbling senselessly in the corners of dusty libraries. For this reason, a deep, geometric understanding of the method has been reserved for the elite brilliant few who have painstakingly decoded the mumblings of their forebears. Nevertheless, the Conjugate Gradient Method is a composite of simple, elegant ideas that almost anyone can understand. Of course, a reader as intelligent as yourself will learn them almost effortlessly. The idea of quadratic forms is introduced and used to derive the methods of Steepest Descent, Conjugate Directions, and Conjugate Gradients. Eigenvectors are explained and used to examine the convergence of the Jacobi Method, Steepest Descent, and Conjugate Gradients. Other topics include preconditioning and the nonlinear Conjugate Gradient Method. I have taken pains to make this article easy to read. Sixty-two illustrations are provided. Dense prose is avoided. Concepts are explained in several different ways. Most equations are coupled with an intuitive interpretation.},
	author = {Shewchuk, Jonathan Richard},
	month = aug,
	year = {1994},
	keywords = {conjugate\_gradient\_method mathematics numerics},
}

@article{young_convergence_1970,
	title = {Convergence {Properties} of the {Symmetric} and {Unsymmetric} {Successive} {Overrelaxation} {Methods} and {Related} {Methods}},
	volume = {24},
	doi = {10.1090/S0025-5718-1970-0281331-4},
	abstract = {The paper is concerned with variants of the successive overrelaxation method (SOR method) for solving the linear system Au = b. Necessary and sufficient conditions are given for the convergence of the symmetric and unsymmetric SOR methods when A is symmetric. The modified SOR, symmetric SOR, and unsymmetric SOR methods are also considered for systems of the form Diu, - CuU2 = bi, - CLU1 + D2u2 = b2 where DI and D2 are square diagonal matrices. Different values of the relaxation factor are used on each set of equations. It is shown that if the matrix corresponding to the Jacobi method of iteration has real eigenvalues and has spectral radius ji {\textless} 1, then the spectral radius of the matrix G associated with any of the methods is not less than that of the ordinary SOR method with w = 2(1 + (1 - TA2)112)-l. Moreover, if the eigenvalues of G are real then no improvement is possible by the use of semi-iterative methods. Introduction. In this paper we study convergence properties of several iterative methods for solving the linear system (1.1) Au- b, where A is a given real nonsingular N X N matrix with nonvanishing diagonal elements, b is a given column vector, and u is a column vector to be determined. Each method can be characterized by an equation},
	journal = {Mathematics of Computation},
	author = {Young, D.},
	year = {1970},
	pages = {793--807},
}

@article{venit_convergence_1975,
	title = {The {Convergence} of {Jacobi} and {Gauss}-{Seidel} {Iteration}},
	volume = {48},
	issn = {0025-570X},
	url = {https://www.jstor.org/stable/2689699},
	doi = {10.2307/2689699},
	number = {3},
	urldate = {2021-06-08},
	journal = {Mathematics Magazine},
	author = {Venit, Stewart},
	year = {1975},
	note = {Publisher: Mathematical Association of America},
	pages = {163--167},
}

@article{paige_modified_2006,
	title = {Modified {Gram}-{Schmidt} ({MGS}), {Least} {Squares}, and {Backward} {Stability} of {MGS}-{GMRES}},
	volume = {28},
	issn = {0895-4798, 1095-7162},
	url = {http://epubs.siam.org/doi/10.1137/050630416},
	doi = {10.1137/050630416},
	abstract = {The generalized minimum residual method (GMRES) [Y. Saad and M. Schultz, SIAM J. Sci. Statist. Comput., 7 (1986), pp. 856â869] for solving linear systems Ax = b is implemented as a sequence of least squares problems involving Krylov subspaces of increasing dimensions. The most usual implementation is modiï¬ed GramâSchmidt GMRES (MGS-GMRES). Here we show that MGS-GMRES is backward stable. The result depends on a more general result on the backward stability of a variant of the MGS algorithm applied to solving a linear least squares problem, and uses other new results on MGS and its loss of orthogonality, together with an important but neglected condition number, and a relation between residual norms and certain singular values.},
	language = {en},
	number = {1},
	urldate = {2021-06-07},
	journal = {SIAM Journal on Matrix Analysis and Applications},
	author = {Paige, Christopher C. and RozloznÃ­k, Miroslav and Strakos, Zdenvek},
	month = jan,
	year = {2006},
	pages = {264--284},
}

@article{arnoldi_principle_1951,
	title = {The principle of minimized iterations in the solution of the matrix eigenvalue problem},
	volume = {9},
	journal = {Quarterly of Applied Mathematics},
	author = {Arnoldi, W.},
	year = {1951},
	pages = {17--29},
}

@book{singh_linear_2014,
	address = {Oxford, United Kingdom},
	title = {Linear algebra : step by step},
	publisher = {Oxford University Press},
	author = {Singh, Kuldeep},
	year = {2014},
}

@techreport{rump_approximate_1990,
	title = {Approximate inverses of almost singular matrices still contain useful information},
	copyright = {http://doku.b.tu-harburg.de/doku/lic\_ohne\_pod.php},
	url = {http://tubdok.tub.tuhh.de/handle/11420/321},
	abstract = {It is well-known that, roughly spoken, a matrix inversion on a computer working in base B with t digits precision in the mantissa applied to a matrix of condition Bk produces approximately t-k correct digits of the inverse. For condition {\textgreater}{\textgreater} Bt one might conclude that an approximate inverse contains virtually useless information. In this note we will show that the latter is not true. An approximate inverse may still be useful, e.g. as a preconditioner. An extended set of examples show that preconditioning a matrix using an approximate inverse (computed in t digits precision) lowers the condition number by a factor Bt. As an example we develop an algorithm for solving systems of linear equations up to condition B2t strictly using t digits precision for all calculations and only allowing for double precision accumulation of inner products.},
	institution = {Techn. Univ. Hamburg-Harburg},
	author = {Rump, Siegfried M.},
	year = {1990},
	doi = {10.15480/882.319},
	note = {Series: Berichte des Forschungsschwerpunktes Informations- und Kommunikationstechnik},
}

@book{kailath_linear_1980,
	address = {Englewood Cliffs, N.J., United States},
	title = {Linear systems},
	publisher = {Prentice-Hall},
	author = {Kailath, Thomas},
	year = {1980},
}

@article{arioli_using_2008,
	title = {Using {FGMRES} to obtain backward stability in mixed precision},
	volume = {33},
	journal = {Electronic Transactions on Numerical Analysis},
	author = {Arioli, Mario and Duff, Iain},
	year = {2008},
}

@article{institute_of_electrical_and_electronics_engineers_ieee_2008,
	title = {{IEEE} {Standard} for {Floating}-{Point} {Arithmetic}},
	doi = {10.1109/IEEESTD.2008.4610935},
	abstract = {This standard specifies interchange and arithmetic formats and methods for binary and decimal floating-point arithmetic in computer programming environments. This standard specifies exception conditions and their default handling. An implementation of a floating-point system conforming to this standard may be realized entirely in software, entirely in hardware, or in any combination of software and hardware. For operations specified in the normative part of this standard, numerical results and exceptions are uniquely determined by the values of the input data, sequence of operations, and destination formats, all under user control.},
	journal = {IEEE Std 754-2008},
	author = {{Institute of Electrical and Electronics Engineers}},
	month = aug,
	year = {2008},
	note = {Conference Name: IEEE Std 754-2008},
	keywords = {754-2008, Floating-point arithmetic, Hardware, IEEE Standards, Microprocessors, NaN, Software, Trademarks, arithmetic, binary, computer, decimal, exponent, floating-point, format, interchange, number, rounding, significand, subnormal},
	pages = {1--70},
}

@article{skeel_scaling_1979,
	title = {Scaling for {Numerical} {Stability} in {Gaussian} {Elimination}},
	volume = {26},
	issn = {0004-5411, 1557-735X},
	url = {https://dl.acm.org/doi/10.1145/322139.322148},
	doi = {10.1145/322139.322148},
	language = {en},
	number = {3},
	urldate = {2021-06-01},
	journal = {Journal of the ACM},
	author = {Skeel, Robert D.},
	month = jul,
	year = {1979},
	pages = {494--526},
}

@book{higham_accuracy_2002,
	address = {USA},
	edition = {2nd},
	title = {Accuracy and {Stability} of {Numerical} {Algorithms}},
	isbn = {0-89871-521-0},
	abstract = {From the Publisher:What is the most accurate way to sum floating point numbers\_\_ \_\_ What are the advantages of IEEE arithmetic\_\_ \_\_ How accurate is Gaussian elimination and what were the key breakthroughs in the development of error analysis for the method\_\_ \_\_ The answers to these and many related questions are included here. This book gives a thorough, up-to-date treatment of the behavior of numerical algorithms in finite precision arithmetic. It combines algorithmic derivations, perturbation theory, and rounding error analysis. Software practicalities are emphasized throughout, with particular reference to LAPACK and MATLAB. The best available error bounds, some of them new, are presented in a unified format with a minimum of jargon. Because of its central role in revealing problem sensitivity and providing error bounds, perturbation theory is treated in detail. Historical perspective and insight are given, with particular reference to the fundamental work of Wilkinson and Turing, and the many quotations provide further information in an accessible format. The book is unique in that algorithmic developments and motivations are given succinctly and implementation details minimized, so that attention can be concentrated on accuracy and stability results. Here, in one place and in a unified notation, is error analysis for most of the standard algorithms in matrix computations. Not since Wilkinson's Rounding Errors in Algebraic Processes (1963) and The Algebraic Eigenvalue Problem (1965) has any volume treated this subject in such depth. A number of topics are treated that are not usually covered in numerical analysis textbooks, including floating point summation, block LU factorization, condition number estimation, the Sylvester equation, powers of matrices, finite precision behavior of stationary iterative methods, Vandermonde systems, and fast matrix multiplication. Although not designed specifically as a textbook, this volume is a suitable reference for an advanced course, and could be used by instructors at all levels as a supplementary text from which to draw examples, historical perspective, statements of results, and exercises (many of which have never before appeared in textbooks). The book is designed to be a comprehensive reference and its bibliography contains more than 1100 references from the research literature. Audience Specialists in numerical analysis as well as computational scientists and engineers concerned about the accuracy of their results will benefit from this book. Much of the book can be understood with only a basic grounding in numerical analysis and linear algebra. About the Author Nicholas J. Higham is a Professor of Applied Mathematics at the University of Manchester, England. He is the author of more than 40 publications and is a member of the editorial boards of the SIAM Journal on Matrix Analysis and Applications and the IMA Journal of Numerical Analysis. His book Handbook of Writing for the Mathematical Sciences was published by SIAM in 1993.},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Higham, Nicholas J.},
	year = {2002},
}

@article{amestoy_improving_2015,
	title = {Improving {Multifrontal} {Methods} by {Means} of {Block} {Low}-{Rank} {Representations}},
	volume = {37},
	journal = {SIAM J. Sci. Comput.},
	author = {Amestoy, P. and Ashcraft, C. and Boiteau, O. and Buttari, A. and LâExcellent, J. and Weisbecker, ClÃ©ment},
	year = {2015},
}

@book{saad_iterative_2003,
	address = {USA},
	edition = {2nd},
	title = {Iterative {Methods} for {Sparse} {Linear} {Systems}},
	isbn = {0-89871-534-2},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Saad, Y.},
	year = {2003},
}

@article{higham_new_2019,
	title = {A {New} {Preconditioner} that {Exploits} {Low}-{Rank} {Approximations} to {Factorization} {Error}},
	volume = {41},
	issn = {1064-8275, 1095-7197},
	url = {https://epubs.siam.org/doi/10.1137/18M1182802},
	doi = {10.1137/18M1182802},
	abstract = {We consider ill-conditioned linear systems Ax = b that are to be solved iteratively, and assume that a low accuracy LU factorization A â LU is available for use in a preconditioner. We have observed that for ill-conditioned matrices A arising in practice, Aâ1 tends to be numerically low rank, that is, it has a small number of large singular values. Importantly, the error matrix E = U â1Lâ1AâI tends to have the same property. To understand this phenomenon we give bounds for the distance from E to a low-rank matrix in terms of the corresponding distance for Aâ1. We then design a novel preconditioner that exploits the low-rank property of the error to accelerate the convergence of iterative methods. We apply this new preconditioner in three diï¬erent contexts ï¬tting our general framework: low ï¬oating-point precision (e.g., half precision) LU factorization, incomplete LU factorization, and block low-rank LU factorization. In numerical experiments with GMRES-based iterative reï¬nement we show that our preconditioner can achieve a signiï¬cant reduction in the number of iterations required to solve a variety of real-life problems.},
	language = {en},
	number = {1},
	urldate = {2021-06-01},
	journal = {SIAM Journal on Scientific Computing},
	author = {Higham, Nicholas J. and Mary, Theo},
	month = jan,
	year = {2019},
	pages = {A59--A82},
}

@inproceedings{haidar_harnessing_2018,
	address = {Dallas, TX, USA},
	title = {Harnessing {GPU} {Tensor} {Cores} for {Fast} {FP16} {Arithmetic} to {Speed} up {Mixed}-{Precision} {Iterative} {Refinement} {Solvers}},
	isbn = {978-1-5386-8384-2},
	url = {https://ieeexplore.ieee.org/document/8665777/},
	doi = {10.1109/SC.2018.00050},
	abstract = {Low-precision floating-point arithmetic is a powerful tool for accelerating scientific computing applications, especially those in artificial intelligence. Here, we present an investigation showing that other high-performance computing (HPC) applications can also harness this power. Specifically, we use the general HPC problem, Ax = b, where A is a large dense matrix, and a double precision (FP64) solution is needed for accuracy. Our approach is based on mixed-precision (FP16---+FP64) iterative refinement, and we generalize and extend prior advances into a framework, for which we develop architecture-specific algorithms and highly tuned implementations. These new methods show how using half-precision Tensor Cores (FP16-TC) for the arithmetic can provide up to 4x speedup. This is due to the performance boost that the FP16-TC provide as well as to the improved accuracy over the classical FP16 arithmetic that is obtained because the GEMM accumulation occurs in FP32 arithmetic.},
	language = {en},
	urldate = {2021-06-01},
	booktitle = {{SC18}: {International} {Conference} for {High} {Performance} {Computing}, {Networking}, {Storage} and {Analysis}},
	publisher = {IEEE},
	author = {Haidar, Azzam and Tomov, Stanimire and Dongarra, Jack and Higham, Nicholas J.},
	month = nov,
	year = {2018},
	pages = {603--613},
}

@article{carson_accelerating_2018,
	title = {Accelerating the {Solution} of {Linear} {Systems} by {Iterative} {Refinement} in {Three} {Precisions}},
	volume = {40},
	issn = {1064-8275, 1095-7197},
	url = {https://epubs.siam.org/doi/10.1137/17M1140819},
	doi = {10.1137/17M1140819},
	abstract = {We propose a general algorithm for solving an n{\textbackslash}times n nonsingular linear system Ax = b based on iterative refinement with three precisions. The working precision is combined with possibly different precisions for solving for the correction term and for computing the residuals. Via rounding error analysis of the algorithm we derive sufficient conditions for convergence and bounds for the attainable forward error and normwise and componentwise backward errors. Our results generalize and unify many existing rounding error analyses for iterative refinement. With single precision as the working precision, we show that by using LU factorization in IEEE half precision as the solver and calculating the residuals in double precision it is possible to solve Ax = b to full single precision accuracy for {\textbackslash}infty -norm condition numbers {\textbackslash}kapa {\textbackslash}infty (A) {\textbackslash}leq  104, with the O(n3) part of the computations carried out entirely in half precision. We show further that by solving the correction equations by GMRES preconditioned by the LU factors the restriction on the condition number can be weakened to {\textbackslash}kapa {\textbackslash}infty (A) {\textbackslash}leq  108, although in general there is no guarantee that GMRES will converge quickly. Taking for comparison a standard Ax = b solver that uses LU factorization in single precision, these results suggest that on architectures for which half precision is efficiently implemented it will be possible to solve certain linear systems Ax = b up to twice as fast and to greater accuracy. Analogous results are given with double precision as the working precision.},
	language = {en},
	number = {2},
	urldate = {2021-06-01},
	journal = {SIAM Journal on Scientific Computing},
	author = {Carson, Erin and Higham, Nicholas J.},
	month = jan,
	year = {2018},
	pages = {A817--A847},
}

@article{rump_inversion_2009,
	title = {Inversion of extremely {Ill}-conditioned matrices in floating-point},
	volume = {26},
	issn = {1868-937X},
	url = {https://doi.org/10.1007/BF03186534},
	doi = {10.1007/BF03186534},
	abstract = {Let ann xn matrixA of floating-point numbers in some format be given. Denote the relative rounding error unit of the given format by eps. AssumeA to be extremely ill-conditioned, that is cond(A) â« epsâ1. In about 1984 I developed an algorithm to calculate an approximate inverse ofA solely using the given floating-point format. The key is a multiplicative correction rather than a Newton-type additive correction. I did not publish it because of lack of analysis. Recently, in [9] a modification of the algorithm was analyzed. The present paper has two purposes. The first is to present reasoning how and why the original algorithm works. The second is to discuss a quite unexpected feature of floating-point computations, namely, that an approximate inverse of an extraordinary ill-conditioned matrix still contains a lot of useful information. We will demonstrate this by inverting a matrix with condition number beyond 10300 solely using double precision. This is a workout of the invited talk at the SCAN meeting 2006 in Duisburg.},
	language = {en},
	number = {2},
	urldate = {2021-06-01},
	journal = {Japan Journal of Industrial and Applied Mathematics},
	author = {Rump, Siegfried M.},
	month = oct,
	year = {2009},
	pages = {249--277},
}

@article{saad_gmres_1986,
	title = {{GMRES}: {A} {Generalized} {Minimal} {Residual} {Algorithm} for {Solving} {Nonsymmetric} {Linear} {Systems}},
	volume = {7},
	issn = {0196-5204},
	shorttitle = {{GMRES}},
	url = {https://epubs.siam.org/doi/abs/10.1137/0907058},
	doi = {10.1137/0907058},
	abstract = {We present an iterative method for solving linear systems, which has the property of minimizing at every step the norm of the residual vector over a Krylov subspace. The algorithm is derived from the Arnoldi process for constructing an \$l\_2 \$-orthogonal basis of Krylov subspaces. It can be considered as a generalization of Paige and Saundersâ MINRES algorithm and is theoretically equivalent to the Generalized Conjugate Residual (GCR) method and to ORTHODIR. The new algorithm presents several advantages over GCR and ORTHODIR.},
	number = {3},
	urldate = {2021-05-28},
	journal = {SIAM Journal on Scientific and Statistical Computing},
	author = {Saad, Youcef and Schultz, Martin H.},
	month = jul,
	year = {1986},
	note = {Publisher: Society for Industrial and Applied Mathematics},
	pages = {856--869},
}

@article{abdelfattah_linear_2016,
	title = {Linear algebra software for large-scale accelerated multicore computing*},
	volume = {25},
	issn = {0962-4929, 1474-0508},
	url = {https://www.cambridge.org/core/journals/acta-numerica/article/linear-algebra-software-for-largescale-accelerated-multicore-computing/D7EC58573BE22E2D6407DB0A330FC5FC},
	doi = {10.1017/S0962492916000015},
	abstract = {Many crucial scientific computing applications, ranging from national security to medical advances, rely on high-performance linear algebra algorithms and technologies, underscoring their importance and broad impact. Here we present the state-of-the-art design and implementation practices for the acceleration of the predominant linear algebra algorithms on large-scale accelerated multicore systems. Examples are given with fundamental dense linear algebra algorithms â from the LU, QR, Cholesky, and LDLT factorizations needed for solving linear systems of equations, to eigenvalue and singular value decomposition (SVD) problems. The implementations presented are readily available via the open-source PLASMA and MAGMA libraries, which represent the next generation modernization of the popular LAPACK library for accelerated multicore systems.To generate the extreme level of parallelism needed for the efficient use of these systems, algorithms of interest are redesigned and then split into well-chosen computational tasks. The task execution is scheduled over the computational components of a hybrid system of multicore CPUs with GPU accelerators and/or Xeon Phi coprocessors, using either static scheduling or light-weight runtime systems. The use of light-weight runtime systems keeps scheduling overheads low, similar to static scheduling, while enabling the expression of parallelism through sequential-like code. This simplifies the development effort and allows exploration of the unique strengths of the various hardware components. Finally, we emphasize the development of innovative linear algebra algorithms using three technologies â mixed precision arithmetic, batched operations, and asynchronous iterations â that are currently of high interest for accelerated multicore systems.},
	language = {en},
	urldate = {2021-05-28},
	journal = {Acta Numerica},
	author = {Abdelfattah, A. and Anzt, H. and Dongarra, J. and Gates, M. and Haidar, A. and Kurzak, J. and Luszczek, P. and Tomov, S. and Yamazaki, I. and YarKhan, A.},
	month = may,
	year = {2016},
	note = {Publisher: Cambridge University Press},
	pages = {1--160},
}

@inproceedings{langou_exploiting_2006,
	title = {Exploiting the {Performance} of 32 bit {Floating} {Point} {Arithmetic} in {Obtaining} 64 bit {Accuracy} ({Revisiting} {Iterative} {Refinement} for {Linear} {Systems})},
	doi = {10.1109/SC.2006.30},
	abstract = {Recent versions of microprocessors exhibit performance characteristics for 32 bit floating point arithmetic (single precision) that is substantially higher than 64 bit floating point arithmetic (double precision). Examples include the Intel's Pentium IV and M processors, AMD's Opteron architectures and the IBM's Cell Broad Engine processor. When working in single precision, floating point operations can be performed up to two times faster on the Pentium and up to ten times faster on the Cell over double precision. The performance enhancements in these architectures are derived by accessing extensions to the basic architecture, such as SSE2 in the case of the Pentium and the vector functions on the IBM Cell. The motivation for this paper is to exploit single precision operations whenever possible and resort to double precision at critical stages while attempting to provide the full double precision results. The results described here are fairly general and can be applied to various problems in linear algebra such as solving large sparse systems, using direct or iterative methods and some eigenvalue problems. There are limitations to the success of this process, such as when the conditioning of the problem exceeds the reciprocal of the accuracy of the single precision computations. In that case the double precision algorithm should be used},
	booktitle = {{SC} '06: {Proceedings} of the 2006 {ACM}/{IEEE} {Conference} on {Supercomputing}},
	author = {Langou, Julie and Langou, Julien and Luszczek, Piotr and Kurzak, Jakub and Buttari, Alfredo and Dongarra, Jack},
	month = nov,
	year = {2006},
	keywords = {Eigenvalues and eigenfunctions, Engines, Floating-point arithmetic, Iterative algorithms, Iterative methods, Lifting equipment, Linear algebra, Linear systems, Microprocessors, Permission},
	pages = {50--50},
}

@book{anderson_lapack_1999,
	address = {Philadelphia, PA},
	edition = {Third},
	title = {{LAPACK} {Users}' {Guide}},
	isbn = {0-89871-447-8 (paperback)},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Anderson, E. and Bai, Z. and Bischof, C. and Blackford, S. and Demmel, J. and Dongarra, J. and Du Croz, J. and Greenbaum, A. and Hammarling, S. and McKenney, A. and Sorensen, D.},
	year = {1999},
}

@article{skeel_iterative_1980,
	title = {Iterative {Refinement} {Implies} {Numerical} {Stability} for {Gaussian} {Elimination}},
	volume = {35},
	doi = {10.2307/2006197},
	journal = {Mathematics of Computation},
	author = {Skeel, Robert},
	year = {1980},
}

@article{higham_iterative_1997,
	title = {Iterative refinement for linear systems and {LAPACK}},
	volume = {17},
	issn = {0272-4979, 1464-3642},
	url = {https://academic.oup.com/imajna/article-lookup/doi/10.1093/imanum/17.4.495},
	doi = {10.1093/imanum/17.4.495},
	language = {en},
	number = {4},
	urldate = {2021-05-26},
	journal = {IMA Journal of Numerical Analysis},
	author = {Higham, N.},
	month = oct,
	year = {1997},
	pages = {495--509},
}

@article{higham_iterative_1991,
	title = {Iterative refinement enhances the stability {ofQR} factorization methods for solving linear equations},
	volume = {31},
	issn = {0006-3835, 1572-9125},
	url = {http://link.springer.com/10.1007/BF01933262},
	doi = {10.1007/BF01933262},
	abstract = {Iterative refinement is a well-known technique for improving the quality of an approximate solution to a linear system. In the traditional usage residuals are computed in extended precision, but more recent work has shown that fixed precision is sufficient to yield benefits for stability. We extend existing results to show that fixed precision iterative refinement renders an arbitrary linear equations solver backward stable in a strong, componentwise sense, under suitable assumptions. Two particular applications involving the QR factorization are discussed in detail: solution of square linear systems and solution of least squares problems. In the former case we show that one step of iterative refinement suffices to produce a small componentwise relative backward error. Our results are weaker for the least squares problem, but again we find that iterative refinement improves a componentwise measure of backward stability. In particular, iterative refinement mitigates the effect of poor row scaling of the coefficient matrix, and so provides an alternative to the use of row interchanges in the Householder QR factorization. A further application of the results is described to fast methods for solving Vandermondelike systems.},
	language = {en},
	number = {3},
	urldate = {2021-05-26},
	journal = {BIT},
	author = {Higham, Nicholas J.},
	month = sep,
	year = {1991},
	pages = {447--468},
}

@article{moler_iterative_1967,
	title = {Iterative {Refinement} in {Floating} {Point}},
	volume = {14},
	issn = {0004-5411, 1557-735X},
	url = {https://dl.acm.org/doi/10.1145/321386.321394},
	doi = {10.1145/321386.321394},
	abstract = {Iter{\textasciitilde}tive refinement reduces the roundoff errors in the computed solution to a system of linear equations. Only one step requires higher precision arithmetic. If sufficiently high precision is used, the final result is shown to be very accurate.},
	language = {en},
	number = {2},
	urldate = {2021-05-26},
	journal = {Journal of the ACM},
	author = {Moler, Cleve B.},
	month = apr,
	year = {1967},
	pages = {316--321},
}

@book{wilkinson_rounding_1963,
	title = {Rounding errors in algebraic processes},
	url = {http://archive.org/details/roundingerrorsin0000wilk},
	abstract = {vi, 161 p; Bibliography: p. 157-158},
	language = {eng},
	urldate = {2021-05-26},
	publisher = {Englewood Cliffs, N.J. : Prentice-Hall},
	author = {Wilkinson, J. H. (James Hardy)},
	collaborator = {{Internet Archive}},
	year = {1963},
	keywords = {Electronic digital computers},
}

@article{sarra_radial_2011,
	title = {Radial basis function approximation methods with extended precision floating point arithmetic},
	volume = {35},
	issn = {0955-7997},
	url = {https://www.sciencedirect.com/science/article/pii/S0955799710001256},
	doi = {10.1016/j.enganabound.2010.05.011},
	abstract = {Radial basis function (RBF) methods that employ infinitely differentiable basis functions featuring a shape parameter are theoretically spectrally accurate methods for scattered data interpolation and for solving partial differential equations. It is also theoretically known that RBF methods are most accurate when the linear systems associated with the methods are extremely ill-conditioned. This often prevents the RBF methods from realizing spectral accuracy in applications. In this work we examine how extended precision floating point arithmetic can be used to improve the accuracy of RBF methods in an efficient manner. RBF methods using extended precision are compared to algorithms that evaluate RBF methods by bypassing the solution of the ill-conditioned linear systems.},
	language = {en},
	number = {1},
	urldate = {2021-05-26},
	journal = {Engineering Analysis with Boundary Elements},
	author = {Sarra, Scott A.},
	month = jan,
	year = {2011},
	keywords = {Extended precision floating point arithmetic, RBF collocation for PDEs, RBF interpolation},
	pages = {68--76},
}

@article{ma_reliable_2017,
	title = {Reliable and efficient solution of genome-scale models of {Metabolism} and macromolecular {Expression}},
	volume = {7},
	copyright = {2017 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/srep40863},
	doi = {10.1038/srep40863},
	abstract = {Constraint-Based Reconstruction and Analysis (COBRA) is currently the only methodology that permits integrated modeling of Metabolism and macromolecular Expression (ME) at genome-scale. Linear optimization computes steady-state flux solutions to ME models, but flux values are spread over many orders of magnitude. Data values also have greatly varying magnitudes. Standard double-precision solvers may return inaccurate solutions or report that no solution exists. Exact simplex solvers based on rational arithmetic require a near-optimal warm start to be practical on large problems (current ME models have 70,000 constraints and variables and will grow larger). We have developed a quadruple-precision version of our linear and nonlinear optimizer MINOS, and a solution procedure (DQQ) involving Double and Quad MINOS that achieves reliability and efficiency for ME models and other challenging problems tested here. DQQ will enable extensive use of large linear and nonlinear models in systems biology and other applications involving multiscale data.},
	language = {en},
	number = {1},
	urldate = {2021-05-26},
	journal = {Scientific Reports},
	author = {Ma, Ding and Yang, Laurence and Fleming, Ronan M. T. and Thiele, Ines and Palsson, Bernhard O. and Saunders, Michael A.},
	month = jan,
	year = {2017},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {40863},
}

@inproceedings{ma_solving_2015,
	address = {Cham},
	series = {Springer {Proceedings} in {Mathematics} \& {Statistics}},
	title = {Solving {Multiscale} {Linear} {Programs} {Using} the {Simplex} {Method} in {Quadruple} {Precision}},
	isbn = {978-3-319-17689-5},
	doi = {10.1007/978-3-319-17689-5_9},
	abstract = {Systems biologists are developing increasingly large models of metabolism and integrated models of metabolism and macromolecular expression. These Metabolic Expression (ME) models lead to sequences of multiscale linear programs for which small solution values of order 10â6 to 10â10 are meaningful. Standard LP solvers do not give sufficiently accurate solutions, and exact simplex solvers are extremely slow. We investigate whether double-precision and quadruple-precision simplex solvers can together achieve reliability at acceptable cost.A double-precision LP solver often provides a reasonably good starting point for a Quad simplex solver. On a range of multiscale examples we find that 34-digit Quad floating-point achieves exceptionally small primal and dual infeasibilities (of order 10â30) when no more than 10â15 is requested. On a significant ME model we also observe robustness in almost all (even small) solution values following relative perturbations of order 10â6 to non-integer data values.Double and Quad Fortran 77 implementations of the linear and nonlinear optimization solver MINOS are available upon request.},
	language = {en},
	booktitle = {Numerical {Analysis} and {Optimization}},
	publisher = {Springer International Publishing},
	author = {Ma, Ding and Saunders, Michael A.},
	editor = {Al-Baali, Mehiddin and Grandinetti, Lucio and Purnama, Anton},
	year = {2015},
	keywords = {Flux balance analysis, Gfortran libquadmath, MINOS, Metabolic expression model, Multiscale linear program, Quadruple precision, Simplex method},
	pages = {223--235},
}

@article{bailey_high-precision_2015,
	title = {High-{Precision} {Arithmetic} in {Mathematical} {Physics}},
	volume = {3},
	doi = {10.3390/math3020337},
	abstract = {For many scientific calculations, particularly those involving empirical data, IEEE 32-bit floating-point arithmetic produces results of sufficient accuracy, while for other applications IEEE 64-bit floating-point is more appropriate. But for some very demanding applications, even higher levels of precision are often required. This article discusses the challenge of high-precision computation, in the context of mathematical physics, and highlights what facilities are required to support future computation, in light of emerging developments in computer architecture.},
	journal = {Mathematics},
	author = {Bailey, David and Borwein, Jonathan},
	month = may,
	year = {2015},
	pages = {337--367},
}

@book{golub_matrix_2013,
	edition = {Fourth},
	title = {Matrix {Computations}},
	isbn = {1-4214-0794-9 978-1-4214-0794-4},
	url = {http://www.cs.cornell.edu/cv/GVL4/golubandvanloan.htm},
	publisher = {JHU Press},
	author = {Golub, Gene H. and van Loan, Charles F.},
	year = {2013},
	keywords = {GvL cauchy circulant courant-fischer determinant dft eigenvalues interlacing linear.algebra matrix pseudoinverse textbook},
}

@book{trefethen_numerical_1997,
	title = {Numerical {Linear} {Algebra}},
	isbn = {0-89871-361-7},
	publisher = {SIAM},
	author = {Trefethen, Lloyd N. and Bau, David},
	year = {1997},
	keywords = {characteristic eigenvalues linear.algebra matrix numerical numerical.analysis polynomial secular.equation textbook},
}

@book{greenbaum_iterative_1997,
	address = {USA},
	title = {Iterative {Methods} for {Solving} {Linear} {Systems}},
	isbn = {0-89871-396-X},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Greenbaum, Anne},
	year = {1997},
}

@article{buttari_mixed_2007,
	title = {Mixed {Precision} {Iterative} {Refinement} {Techniques} for the {Solution} of {Dense} {Linear} {Systems}},
	volume = {21},
	issn = {1094-3420},
	url = {https://doi.org/10.1177/1094342007084026},
	doi = {10.1177/1094342007084026},
	abstract = {By using a combination of 32-bit and 64-bit floating point arithmetic, the performance of many dense and sparse linear algebra algorithms can be significantly enhanced while maintaining the 64-bit accuracy of the resulting solution. The approach presented here can apply not only to conventional processors but also to exotic technologies such as Field Programmable Gate Arrays (FPGA), Graphical Processing Units (GPU), and the Cell BE processor. Results on modern processor architectures and the Cell BE are presented.},
	language = {en},
	number = {4},
	urldate = {2021-05-21},
	journal = {The International Journal of High Performance Computing Applications},
	author = {Buttari, Alfredo and Dongarra, Jack and Langou, Julie and Langou, Julien and Luszczek, Piotr and Kurzak, Jakub},
	month = nov,
	year = {2007},
	note = {Publisher: SAGE Publications Ltd STM},
	keywords = {Cholesky factorization, LU, iterative, mixed-precision, refinement},
	pages = {457--466},
}

@book{davis_direct_2006,
	address = {USA},
	title = {Direct {Methods} for {Sparse} {Linear} {Systems} ({Fundamentals} of {Algorithms} 2)},
	isbn = {0-89871-613-6},
	publisher = {Society for Industrial and Applied Mathematics},
	author = {Davis, Timothy A.},
	year = {2006},
}

@article{carson_new_2017,
	title = {A {New} {Analysis} of {Iterative} {Refinement} and {Its} {Application} to {Accurate} {Solution} of {Ill}-{Conditioned} {Sparse} {Linear} {Systems}},
	volume = {39},
	issn = {1064-8275, 1095-7197},
	url = {https://epubs.siam.org/doi/10.1137/17M1122918},
	doi = {10.1137/17M1122918},
	abstract = {Iterative reï¬nement is a long-standing technique for improving the accuracy of a computed solution to a nonsingular linear system Ax = b obtained via LU factorization. It makes use of residuals computed in extra precision, typically at twice the working precision, and existing results guarantee convergence if the matrix A has condition number safely less than the reciprocal of the unit roundoï¬, u. We identify a mechanism that allows iterative reï¬nement to produce solutions with normwise relative error of order u to systems with condition numbers of order uâ1 or larger, provided that the update equation is solved with a relative error suï¬ciently less than 1. A new rounding error analysis is given, and its implications are analyzed. Building on the analysis, we develop a GMRES (generalized minimal residual)-based iterative reï¬nement method (GMRES-IR) that makes use of the computed LU factors as preconditioners. GMRES-IR exploits the fact that even if A is extremely ill conditioned the LU factors contain enough information that preconditioning can greatly reduce the condition number of A. Our rounding error analysis and numerical experiments show that GMRES-IR can succeed where standard reï¬nement fails, and that it can provide accurate solutions to systems with condition numbers of order uâ1 and greater. Indeed, in our experiments with such matricesâboth random and from the University of Florida Sparse Matrix CollectionâGMRES-IR yields a normwise relative error of order u in at most three steps in every case.},
	language = {en},
	number = {6},
	urldate = {2021-05-21},
	journal = {SIAM Journal on Scientific Computing},
	author = {Carson, Erin and Higham, Nicholas J.},
	month = jan,
	year = {2017},
	pages = {A2834--A2856},
}

@book{strang_introduction_2009,
	address = {Wellesley, MA},
	edition = {Fifth},
	title = {Introduction to {Linear} {Algebra}},
	isbn = {978-0-9802327-1-4 0-9802327-1-6 978-0-9802327-2-1 0-9802327-2-4 978-81-7596-811-0 81-7596-811-7},
	abstract = {Book Description: Gilbert Strang's textbooks have changed the entire approach to learning linear algebra â away from abstract vector spaces to specific examples of the four fundamental subspaces: the column space and nullspace of A and A'. Introduction to Linear Algebra, Fourth Edition includes challenge problems to complement the review problems that have been highly praised in previous editions. The basic course is followed by seven applications: differential equations, engineering, graph theory, statistics, Fourier methods and the FFT, linear programming, and computer graphics. Thousands of teachers in colleges and universities and now high schools are using this book, which truly explains this crucial subject.},
	publisher = {Wellesley-Cambridge Press},
	author = {Strang, Gilbert},
	year = {2009},
	keywords = {linear.algebra matrix strang textbook},
}
